{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bce4334",
   "metadata": {},
   "source": [
    "# Prepare and transform data in the lakehouse (PySpark)\n",
    "\n",
    "This is a companion notebook for the Microsoft Learn tutorial: https://learn.microsoft.com/fabric/data-engineering/tutorial-lakehouse-data-preparation\n",
    "The website tutorial flow follows Path 1 in this notebook.\n",
    "\n",
    "This notebook includes two execution paths:\n",
    "- Path 1: Lakehouse schemas enabled (`Tables/dbo/...`) — this is the supported path for the tutorial article.\n",
    "- Path 2: Lakehouse schemas not enabled (`wwilakehouse....`) — use this alternate path when schemas are not enabled in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e92114",
   "metadata": {},
   "source": [
    "## Path 1 - Lakehouse schemas enabled (tutorial-supported path)\n",
    "### Create Delta tables\n",
    "Run these cells to create Delta tables from raw data using schema-qualified paths (`Tables/dbo/...`).\n",
    "\n",
    "#### Cell 1 - Spark session configuration\n",
    "This cell enables two Fabric features that optimize how data is written and read in subsequent cells. V-order optimizes parquet layout for faster reads and better compression. Optimize Write reduces the number of files written and increases individual file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da24275",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48222a05",
   "metadata": {},
   "source": [
    "#### Cell 2 - Fact - Sale\n",
    "This cell reads raw parquet data from `Files/wwi-raw-data/full/fact_sale_1y_full`, adds date part columns (`Year`, `Quarter`, and `Month`), and writes `fact_sale` as a Delta table partitioned by `Year` and `Quarter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84655495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "\n",
    "df = spark.read.format(\"parquet\").load(\"Files/wwi-raw-data/full/fact_sale_1y_full\")\n",
    "df = df.withColumn(\"Year\", year(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn(\"Quarter\", quarter(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn(\"Month\", month(col(\"InvoiceDateKey\")))\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\", \"Quarter\").save(\"Tables/dbo/fact_sale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737fac5",
   "metadata": {},
   "source": [
    "#### Cell 3 - Dimensions\n",
    "This cell reads the five dimension parquet datasets and writes them as Delta tables (`dimension_city`, `dimension_customer`, `dimension_date`, `dimension_employee`, and `dimension_stock_item`) under `Tables/dbo/...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data_from_source(table_name):\n",
    "    df = spark.read.format(\"parquet\").load(\"Files/wwi-raw-data/full/\" + table_name)\n",
    "    if \"Photo\" in df.columns:\n",
    "        df = df.drop(\"Photo\")\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/dbo/\" + table_name)\n",
    "\n",
    "full_tables = [\n",
    "    \"dimension_city\",\n",
    "    \"dimension_customer\",\n",
    "    \"dimension_date\",\n",
    "    \"dimension_employee\",\n",
    "    \"dimension_stock_item\",\n",
    "]\n",
    "\n",
    "for table in full_tables:\n",
    "    load_full_data_from_source(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4397f6f",
   "metadata": {},
   "source": [
    "### Transform data for business aggregates\n",
    "Run the transformation cells to create aggregate outputs for reporting in the schema-enabled path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271d771",
   "metadata": {},
   "source": [
    "#### Cell 4 - Load source tables\n",
    "This cell loads the source Delta tables needed for business aggregate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sale = spark.read.format(\"delta\").load(\"Tables/dbo/fact_sale\")\n",
    "df_dimension_date = spark.read.format(\"delta\").load(\"Tables/dbo/dimension_date\")\n",
    "df_dimension_city = spark.read.format(\"delta\").load(\"Tables/dbo/dimension_city\")\n",
    "df_dimension_employee = spark.read.format(\"delta\").load(\"Tables/dbo/dimension_employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfde460",
   "metadata": {},
   "source": [
    "#### Cell 5 - Aggregate sale by date and city\n",
    "This cell computes monthly sales totals by city and writes the result to `aggregate_sale_by_date_city`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44851168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_by_date_city = (\n",
    "    df_fact_sale.alias(\"sale\")\n",
    "    .join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\")\n",
    "    .join(df_dimension_city.alias(\"city\"), df_fact_sale.CityKey == df_dimension_city.CityKey, \"inner\")\n",
    "    .select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\")\n",
    "    .sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\n",
    "    .withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\n",
    "    .withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\n",
    "    .withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\n",
    "    .orderBy(\"date.Date\", \"city.StateProvince\", \"city.City\")\n",
    ")\n",
    "\n",
    "sale_by_date_city.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/dbo/aggregate_sale_by_date_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba324b",
   "metadata": {},
   "source": [
    "#### Cell 6 - Aggregate sale by date and employee\n",
    "This cell computes monthly sales totals by employee and writes the result to `aggregate_sale_by_date_employee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_by_date_employee = (\n",
    "    df_fact_sale.alias(\"sale\")\n",
    "    .join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\")\n",
    "    .join(df_dimension_employee.alias(\"employee\"), df_fact_sale.SalespersonKey == df_dimension_employee.EmployeeKey, \"inner\")\n",
    "    .select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"employee.PreferredName\", \"employee.Employee\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"employee.PreferredName\", \"employee.Employee\")\n",
    "    .sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\n",
    "    .withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\n",
    "    .withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\n",
    "    .withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\n",
    "    .orderBy(\"date.Date\", \"employee.PreferredName\", \"employee.Employee\")\n",
    ")\n",
    "\n",
    "sale_by_date_employee.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/dbo/aggregate_sale_by_date_employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969dc23",
   "metadata": {},
   "source": [
    "## Path 2 - Lakehouse schemas not enabled (alternate path)\n",
    "### Create Delta tables\n",
    "Run these cells to create Delta tables from raw data using non-schema table names (`wwilakehouse....`).\n",
    "\n",
    "#### Cell 1 - Spark session configuration\n",
    "This cell enables two Fabric features that optimize how data is written and read in subsequent cells. V-order optimizes parquet layout for faster reads and better compression. Optimize Write reduces the number of files written and increases individual file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4197fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38391d9",
   "metadata": {},
   "source": [
    "#### Cell 2 - Fact - Sale\n",
    "This cell reads raw parquet data from `Files/wwi-raw-data/full/fact_sale_1y_full`, adds date part columns (`Year`, `Quarter`, and `Month`), and writes `wwilakehouse.fact_sale` as a Delta table partitioned by `Year` and `Quarter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f359e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "\n",
    "df = spark.read.format(\"parquet\").load(\"Files/wwi-raw-data/full/fact_sale_1y_full\")\n",
    "df = df.withColumn(\"Year\", year(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn(\"Quarter\", quarter(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn(\"Month\", month(col(\"InvoiceDateKey\")))\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\", \"Quarter\").saveAsTable(\"wwilakehouse.fact_sale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5bd64",
   "metadata": {},
   "source": [
    "#### Cell 3 - Dimensions\n",
    "This cell reads the five dimension parquet datasets and writes them as Delta tables (`dimension_city`, `dimension_customer`, `dimension_date`, `dimension_employee`, and `dimension_stock_item`) under `wwilakehouse....`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data_from_source_no_schema(table_name):\n",
    "    df = spark.read.format(\"parquet\").load(\"Files/wwi-raw-data/full/\" + table_name)\n",
    "    if \"Photo\" in df.columns:\n",
    "        df = df.drop(\"Photo\")\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"wwilakehouse.\" + table_name)\n",
    "\n",
    "full_tables = [\n",
    "    \"dimension_city\",\n",
    "    \"dimension_customer\",\n",
    "    \"dimension_date\",\n",
    "    \"dimension_employee\",\n",
    "    \"dimension_stock_item\",\n",
    "]\n",
    "\n",
    "for table in full_tables:\n",
    "    load_full_data_from_source_no_schema(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55479cd3",
   "metadata": {},
   "source": [
    "### Transform data for business aggregates\n",
    "Run the transformation cells to create aggregate outputs for reporting in the non-schema path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57e080",
   "metadata": {},
   "source": [
    "#### Cell 4 - Load source tables\n",
    "This step prepares the source tables for aggregation in Path 2. For PySpark, this cell explicitly loads the Delta source tables used in the transformation code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4870ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sale = spark.read.table(\"wwilakehouse.fact_sale\")\n",
    "df_dimension_date = spark.read.table(\"wwilakehouse.dimension_date\")\n",
    "df_dimension_city = spark.read.table(\"wwilakehouse.dimension_city\")\n",
    "df_dimension_employee = spark.read.table(\"wwilakehouse.dimension_employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efc27c",
   "metadata": {},
   "source": [
    "#### Cell 5 - Aggregate sale by date and city\n",
    "This cell computes monthly sales totals by city and writes the result to `wwilakehouse.aggregate_sale_by_date_city`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_by_date_city = (\n",
    "    df_fact_sale.alias(\"sale\")\n",
    "    .join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\")\n",
    "    .join(df_dimension_city.alias(\"city\"), df_fact_sale.CityKey == df_dimension_city.CityKey, \"inner\")\n",
    "    .select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\")\n",
    "    .sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\n",
    "    .withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\n",
    "    .withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\n",
    "    .withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\n",
    "    .orderBy(\"date.Date\", \"city.StateProvince\", \"city.City\")\n",
    ")\n",
    "\n",
    "sale_by_date_city.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fdb69a",
   "metadata": {},
   "source": [
    "#### Cell 6 - Aggregate sale by date and employee\n",
    "This cell computes monthly sales totals by employee and writes the result to `wwilakehouse.aggregate_sale_by_date_employee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_by_date_employee = (\n",
    "    df_fact_sale.alias(\"sale\")\n",
    "    .join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\")\n",
    "    .join(df_dimension_employee.alias(\"employee\"), df_fact_sale.SalespersonKey == df_dimension_employee.EmployeeKey, \"inner\")\n",
    "    .select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"employee.PreferredName\", \"employee.Employee\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"employee.PreferredName\", \"employee.Employee\")\n",
    "    .sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\n",
    "    .withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\n",
    "    .withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\n",
    "    .withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\n",
    "    .withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\n",
    "    .orderBy(\"date.Date\", \"employee.PreferredName\", \"employee.Employee\")\n",
    ")\n",
    "\n",
    "sale_by_date_employee.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_employee\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
