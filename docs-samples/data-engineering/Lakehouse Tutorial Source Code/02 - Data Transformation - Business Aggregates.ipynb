{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Spark session configuration\n",
    "As in the previous notebook, this cell enables V-order and Optimize Write for the Spark session.\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Approach #1 - sale_by_date_city\n",
    "This cell loads the `fact_sale`, `dimension_date`, and `dimension_city` Delta tables into PySpark dataframes, preparing the data for joining and aggregation in the next cell.\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_fact_sale = spark.read.table(\"wwilakehouse.fact_sale\") \n",
    "df_dimension_date = spark.read.table(\"wwilakehouse.dimension_date\")\n",
    "df_dimension_city = spark.read.table(\"wwilakehouse.dimension_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Join and aggregate by date and city\n",
    "This cell joins the three dataframes on their key columns, selects date, city, and sales fields, then groups and sums sales totals and profit by date and city.\n",
    "It writes the result as the `aggregate_sale_by_date_city` Delta table.\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sale_by_date_city = df_fact_sale.alias(\"sale\") \\\n",
    ".join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\") \\\n",
    ".join(df_dimension_city.alias(\"city\"), df_fact_sale.CityKey == df_dimension_city.CityKey, \"inner\") \\\n",
    ".select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
    ".groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\")\\\n",
    ".sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
    ".withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\\\n",
    ".withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\\\n",
    ".withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\\\n",
    ".withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\\\n",
    ".orderBy(\"date.Date\", \"city.StateProvince\", \"city.City\")\n",
    "\n",
    "sale_by_date_city.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Approach #2 - sale_by_date_employee\n",
    "This cell uses Spark SQL to create a temporary view called `sale_by_date_employee` by joining `fact_sale`, `dimension_date`, and `dimension_employee`, then aggregating sales metrics by date and employee.\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "sparksql",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW sale_by_date_employee\n",
    "AS\n",
    "SELECT\n",
    "\tDD.Date, DD.CalendarMonthLabel\n",
    "    , DD.Day, DD.ShortMonth Month, CalendarYear Year\n",
    "\t,DE.PreferredName, DE.Employee\n",
    "\t,SUM(FS.TotalExcludingTax) SumOfTotalExcludingTax\n",
    "\t,SUM(FS.TaxAmount) SumOfTaxAmount\n",
    "\t,SUM(FS.TotalIncludingTax) SumOfTotalIncludingTax\n",
    "\t,SUM(Profit) SumOfProfit \n",
    "FROM wwilakehouse.fact_sale FS\n",
    "INNER JOIN wwilakehouse.dimension_date DD ON FS.InvoiceDateKey = DD.Date\n",
    "INNER JOIN wwilakehouse.dimension_Employee DE ON FS.SalespersonKey = DE.EmployeeKey\n",
    "GROUP BY DD.Date, DD.CalendarMonthLabel, DD.Day, DD.ShortMonth, DD.CalendarYear, DE.PreferredName, DE.Employee\n",
    "ORDER BY DD.Date ASC, DE.PreferredName ASC, DE.Employee ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Write aggregate_sale_by_date_employee\n",
    "This cell reads from the `sale_by_date_employee` temporary view created in the previous cell and writes the results as the `aggregate_sale_by_date_employee` Delta table.\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sale_by_date_employee = spark.sql(\"SELECT * FROM sale_by_date_employee\")\n",
    "sale_by_date_employee.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_employee\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    },
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
