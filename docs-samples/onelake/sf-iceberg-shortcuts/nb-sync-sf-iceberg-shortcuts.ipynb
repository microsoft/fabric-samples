{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "**About this notebook**\n",
    "\n",
    "This notebook scans a Snowflake database schema for Iceberg tables, and creates corresponding [Iceberg shortcuts](https://learn.microsoft.com/fabric/onelake/onelake-iceberg-tables) in a Fabric Lakehouse.\n",
    "\n",
    "**Before you run this notebook**\n",
    "\n",
    "Using Microsoft Fabric, create a cloud connection for storage account (ADLS Gen2). The cloud connection must be pointing to the same ADLS Gen2 account which is used by Snowflake to host iceberg tables as an [external volume](https://docs.snowflake.com/user-guide/tables-iceberg). \n",
    "\n",
    "Once you have configured a connection, you can use its Connection ID (unique identifier) across various Microsoft Fabric experiences such as external shortcuts, Data Pipelines and Dataflow Gen2. In this notebook, you will use Connection ID to create external shortcuts. For more information on cloud connection and how to obtain Connection ID, refer to [this link](https://learn.microsoft.com/fabric/data-factory/connector-azure-data-lake-storage-gen2). You can also create a cloud connection using [Fabric REST API](https://learn.microsoft.com/fabric/data-factory/connector-rest).\n",
    "\n",
    "**Input parameters**\n",
    "- [Connection ID](https://learn.microsoft.com/fabric/data-factory/data-source-management#retrieve-a-data-source-connection-id) - You can retrieve this either via API or Fabric web interface. Connection ID is used to create shortcuts once the notebook generates list of iceberg tables.\n",
    "- Storage account name - This is the storage account used by Snowflake database to store Apache Iceberg tables.\n",
    "- Name of Snowflake database\n",
    "- Name of Snowflake schema \n",
    "- Username and password to access Snowflake database. This is used to scan metadata and build a list of iceberg tables on Snowflake. Consider using credentials which have permission to scan metadata and generate this list. You do not need admin privileges for this.\n",
    "- [Snowflake account identifier](https://docs.snowflake.com/user-guide/admin-account-identifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Snowflake connector\n",
    "!pip install -q snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import requests, json\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import time\n",
    "from datetime import datetime\n",
    "import notebookutils\n",
    "import sempy.fabric as fabric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "Use the cell below to set variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with connection ID of ADLS \n",
    "connection_id = \"<replace with connection id>\"\n",
    "\n",
    "# Storage account name on ADLS\n",
    "storage_account = \"<ADLS storage account name used with Snowflake to store iceberg tables>\"\n",
    "\n",
    "# Snowflake variables\n",
    "# Name of Snowflake database\n",
    "catalog = '<replace with Snowflake database name>'\n",
    "\n",
    "# Name of schema on Snowflake database where iceberg tables are deployed\n",
    "db = '<replace with Snowflake schema name>'\n",
    "\n",
    "\n",
    "# Attention: This sample defines username and password inline. For production scenarios, please use Azure Key Vault (AKV) for secrets management.\n",
    "\n",
    "# Username for Snowflake\n",
    "user = \"<replace with Snowflake user>\"\n",
    "\n",
    "# Password for Snowflake\n",
    "password = \"<replace with password for Snowflake user>\"\n",
    "\n",
    "# Snowflake account\n",
    "account_snow = \"<Snowflake account identifier>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "The cells below this section do not require any user inputs and should just run once variables have been configured in the cell above. You can modify the code (below) to enhance capabilities of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric - workspace and lakehouse IDs \n",
    "# This is the workspace / lakehouse where iceberg shortcuts will be created\n",
    "lakehouse_id = fabric.get_lakehouse_id()\n",
    "workspace_id = fabric.get_notebook_workspace_id()\n",
    "\n",
    "# Sync configuration\n",
    "sync_config = {\n",
    "    \"batch_size\": 10,\n",
    "    \"max_retries\": 3,\n",
    "    \"retry_delay\": 5,\n",
    "    \"shortcut_conflict_policy\": \"GenerateUniqueName\",  # Changed from Abort\n",
    "    \"sync_metadata\": True,\n",
    "    \"create_sync_log\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table discovery and metadata extraction from Snowflake\n",
    "def get_snowflake_iceberg_metadata():\n",
    "    \"\"\"Extract Iceberg table metadata from Snowflake - Unity Catalog style\"\"\"\n",
    "    conn = snowflake.connector.connect(\n",
    "        user = user ,\n",
    "        account= account_snow, \n",
    "        password = password ,\n",
    "        insecure_mode=True\n",
    "    )\n",
    "    \n",
    "    snow = conn.cursor()\n",
    "    snow.execute(f\"USE {catalog}.{db}\")\n",
    "    \n",
    "    df = snow.execute(f'''\n",
    "    SELECT table_name \n",
    "    FROM INFORMATION_SCHEMA.TABLES \n",
    "    WHERE table_schema = '{db}' \n",
    "    AND IS_ICEBERG = 'YES'\n",
    "''').fetch_pandas_all()\n",
    "\n",
    "    # Process all tables in single query using UNION ALL\n",
    "    table_list = \"', '\".join(df['TABLE_NAME'].tolist())\n",
    "    results_df = snow.execute(f'''\n",
    "        SELECT \n",
    "            catalog, schema_name,\n",
    "            table_name as name,\n",
    "            CASE \n",
    "                WHEN CONTAINS(info_json, '\"metadataLocation\"') THEN\n",
    "                    REGEXP_REPLACE(\n",
    "                        SPLIT_PART(\n",
    "                            PARSE_JSON(info_json):metadataLocation::STRING, \n",
    "                            '/metadata/', 1\n",
    "                        ), \n",
    "                        '^(abfss://|azure://)[^/]+\\\\.(blob\\\\.core\\\\.windows\\\\.net|dfs\\\\.core\\\\.windows\\\\.net)/', ''\n",
    "                    ) || '/'\n",
    "                ELSE ''\n",
    "            END as location\n",
    "        FROM (\n",
    "            {\" UNION ALL \".join([f\"SELECT'{catalog}' as catalog,'{db}' as schema_name, '{tbl}' as table_name, SYSTEM$GET_ICEBERG_TABLE_INFORMATION('{tbl}') as info_json\" for tbl in df['TABLE_NAME']])}\n",
    "        )\n",
    "        WHERE info_json IS NOT NULL\n",
    "    ''').fetch_pandas_all()\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneLake shortcut API to create shortcuts based on iceberg table list.\n",
    "\n",
    "class FabricSyncManager:\n",
    "    def __init__(self, workspace_id, lakehouse_id, storage_account, connection_id):\n",
    "        self.workspace_id = workspace_id\n",
    "        self.lakehouse_id = lakehouse_id\n",
    "        self.storage_account = storage_account\n",
    "        self.connection_id = connection_id\n",
    "        self.api_base = \"api.fabric.microsoft.com/v1\"\n",
    "        self.sync_log = []\n",
    "        \n",
    "    def get_auth_headers(self):\n",
    "        return {\n",
    "            \"Authorization\": \"Bearer \" + notebookutils.credentials.getToken(\"pbi\"),\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def invoke_api(self, method, uri, payload=None):\n",
    "        \"\"\"Enhanced API call with retry logic\"\"\"\n",
    "        url = f\"https://{self.api_base}/{uri}\"\n",
    "        \n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=sync_config[\"max_retries\"], \n",
    "            backoff_factor=sync_config[\"retry_delay\"], \n",
    "            status_forcelist=[429, 502, 503, 504]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "        \n",
    "        try:\n",
    "            response = session.request(\n",
    "                method, url, \n",
    "                headers=self.get_auth_headers(), \n",
    "                json=payload, \n",
    "                timeout=240\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'status_code': response.status_code,\n",
    "                'response': response.text,\n",
    "                'success': response.status_code < 400\n",
    "            }\n",
    "            \n",
    "        except requests.RequestException as ex:\n",
    "            return {\n",
    "                'status_code': 0,\n",
    "                'response': str(ex),\n",
    "                'success': False\n",
    "            }\n",
    "    \n",
    "    def check_existing_shortcuts(self):\n",
    "        \"\"\"Check existing shortcuts in lakehouse\"\"\"\n",
    "        uri = f\"workspaces/{self.workspace_id}/items/{self.lakehouse_id}/shortcuts\"\n",
    "        result = self.invoke_api(\"GET\", uri)\n",
    "        \n",
    "        if result['success']:\n",
    "            shortcuts_data = json.loads(result['response'])\n",
    "            return {s['name']: s for s in shortcuts_data.get('value', [])}\n",
    "        return {}\n",
    "    \n",
    "    def create_shortcut(self, table_info):\n",
    "        \"\"\"Create shortcut with enhanced metadata\"\"\"\n",
    "        shortcut_name = table_info['NAME']\n",
    "        \n",
    "        payload = {\n",
    "            \"path\": \"Tables/dbo\",\n",
    "            \"name\": shortcut_name,\n",
    "            \"target\": {\n",
    "                \"adlsGen2\": {\n",
    "                    \"location\": f\"https://{self.storage_account}.dfs.core.windows.net/\",\n",
    "                    \"subpath\": table_info['LOCATION'],\n",
    "                    \"connectionId\": self.connection_id\n",
    "                }\n",
    "            },\n",
    "            \"description\": f\"Snowflake Iceberg table: {table_info['CATALOG']}.{table_info['SCHEMA_NAME']}.{table_info['NAME']}\"\n",
    "        }\n",
    "        \n",
    "        uri = f\"workspaces/{self.workspace_id}/items/{self.lakehouse_id}/shortcuts?shortcutConflictPolicy={sync_config['shortcut_conflict_policy']}\"\n",
    "        \n",
    "        result = self.invoke_api(\"POST\", uri, payload)\n",
    "        \n",
    "        # Log the sync operation\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'name': shortcut_name,\n",
    "            'operation': 'create_shortcut',\n",
    "            'status': 'success' if result['success'] else 'failed',\n",
    "            'details': result\n",
    "        }\n",
    "        self.sync_log.append(log_entry)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def sync_tables(self, tables_df):\n",
    "        \"\"\"Sync all tables with batch processing\"\"\"\n",
    "        print(f\"Starting sync of {len(tables_df)} tables...\")\n",
    "        \n",
    "        # Check existing shortcuts\n",
    "        existing_shortcuts = self.check_existing_shortcuts()\n",
    "        print(f\"Found {len(existing_shortcuts)} existing shortcuts\")\n",
    "        \n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for index, row in tables_df.iterrows():\n",
    "            name = row['NAME']\n",
    "            \n",
    "            print(f\"Processing {index + 1}/{len(tables_df)}: {name}\")\n",
    "            \n",
    "            if name in existing_shortcuts:\n",
    "                print(f\"  - Shortcut already exists, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            if not row['LOCATION']:\n",
    "                print(f\"  - No data location found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            result = self.create_shortcut(row)\n",
    "            \n",
    "            if result['success']:\n",
    "                print(f\"  ✓ Successfully created shortcut\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"  ✗ Failed: {result['response']}\")\n",
    "                error_count += 1\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"\\nSync completed: {success_count} success, {error_count} errors\")\n",
    "        return self.sync_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "",
   "metadata": {},
   "source": [
    "Bringing it all together - this last cell calls various functions to create shortcuts in Microsoft Fabric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sync\n",
    "print(\"=== Snowflake to Fabric Sync ===\")\n",
    "print(f\"Catalog: {catalog}, Schema: {db}\")\n",
    "print(f\"Target Lakehouse: {lakehouse_id}\")\n",
    "print()\n",
    "\n",
    "# Get table metadata\n",
    "print(\"1. Extracting Iceberg table metadata from Snowflake...\")\n",
    "tables_df = get_snowflake_iceberg_metadata()\n",
    "print(f\"Found {len(tables_df)} Iceberg tables\")\n",
    "\n",
    "if not tables_df.empty:\n",
    "    display(tables_df[['CATALOG', 'SCHEMA_NAME', 'NAME', 'LOCATION']])\n",
    "    \n",
    "    # Initialize sync\n",
    "    print(\"\\n2. Initializing ...\")\n",
    "    sync_manager = FabricSyncManager(\n",
    "        workspace_id, lakehouse_id, \n",
    "        storage_account, connection_id\n",
    "    )\n",
    "    \n",
    "    # Execute sync\n",
    "    print(\"\\n3. Syncing tables to Fabric...\")\n",
    "    sync_log = sync_manager.sync_tables(tables_df)\n",
    "    \n",
    "    # Display sync results\n",
    "    print(\"\\n4. Sync Summary:\")\n",
    "    sync_df = pd.DataFrame(sync_log)\n",
    "    if not sync_df.empty:\n",
    "        display(sync_df[['timestamp', 'name', 'operation', 'status']])\n",
    "        \n",
    "        # Save sync log for audit\n",
    "        sync_df.to_json(f\"sync_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "        print(\"Sync log saved for audit purposes\")\n",
    "    \n",
    "else:\n",
    "    print(\"No Iceberg tables found to sync\")\n",
    "\n",
    "print(\"\\n=== Sync Complete ===\")"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "",
    "default_lakehouse_name": "",
    "default_lakehouse_workspace_id": "",
    "known_lakehouses": [
     {
      "id": ""
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": null,
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_ignore_dictionary": [
     "lakehouse"
    ],
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
