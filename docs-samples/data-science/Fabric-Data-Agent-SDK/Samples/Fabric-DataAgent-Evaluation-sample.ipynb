{"cells":[{"cell_type":"markdown","source":["# Evaluate a Fabric Data Agent\n","\n","In this notebook, we'll walk through how to evaluate a Fabric Data Agent using the `fabric-data-agent-sdk`. We'll cover the full workflow, including:\n","\n","- âœ… Creating a new Data Agent from the SDK\n","- ðŸ—‚ï¸ Adding data sources and selecting relevant tables\n","- ðŸ“‹ Defining a ground truth dataset with questions and expected answers\n","- ðŸ§ª Running an automated evaluation to compare actual vs. expected responses\n","- ðŸ“ˆ Reviewing evaluation summaries and detailed results\n","\n","This end-to-end example is designed to help you validate the accuracy of your Data Agent and iterate on improvements with structured feedback.\n","\n","Let's get started!\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"61868986-8ba7-4e5e-a5dd-a72fe0764fed"},{"cell_type":"markdown","source":["> **Prerequisite: Load Sample Data into the Lakehouse**\n","\n","Before running this notebook, make sure youâ€™ve created a Lakehouse and loaded the sample **AdventureWorks** dataset.\n","\n","Follow the steps in the official guide to create the Lakehouse and populate it with sample tables:\n","ðŸ‘‰ [Create a Lakehouse with AdventureWorksLH](https://learn.microsoft.com/en-us/fabric/data-science/data-agent-scenario#create-a-lakehouse-with-adventureworkslh)\n","\n","This ensures that the required tables are available for your Data Agent to access during evaluation.\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"400b26cb-5856-4570-9244-bfd1cf47d0b1"},{"cell_type":"markdown","source":["## Install Fabric Data Agent SDK\n","\n","Before we begin, install the latest version of the `fabric-data-agent-sdk`. This SDK provides all the tools you need to create, configure, and evaluate your Data Agent programmatically.\n","\n","Run the following cell to install or upgrade the SDK in your notebook environment:\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"d62e14e7-39da-4bd1-a8f1-5f723b20b804"},{"cell_type":"code","source":["%pip install -U fabric-data-agent-sdk"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:11:51.4448149Z","session_start_time":null,"execution_start_time":"2025-05-06T01:11:51.4456846Z","execution_finish_time":"2025-05-06T01:11:54.6750773Z","parent_msg_id":"b3ac5fc7-8abe-475e-87db-74ec21c5b528"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fabric-data-agent-sdk in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (0.1.2a0)\r\nRequirement already satisfied: semantic-link-sempy>=0.8.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from fabric-data-agent-sdk) (0.9.3)\r\nRequirement already satisfied: openai>=1.57.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from fabric-data-agent-sdk) (1.77.0)\r\nRequirement already satisfied: httpx==0.27.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from fabric-data-agent-sdk) (0.27.2)\r\nRequirement already satisfied: anyio in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk) (4.8.0)\r\nRequirement already satisfied: certifi in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk) (2024.7.4)\r\nRequirement already satisfied: httpcore==1.* in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk) (1.0.9)\r\nRequirement already satisfied: idna in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk) (3.10)\r\nRequirement already satisfied: sniffio in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from httpx==0.27.2->fabric-data-agent-sdk) (1.3.1)\r\nRequirement already satisfied: h11>=0.16 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx==0.27.2->fabric-data-agent-sdk) (0.16.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from openai>=1.57.0->fabric-data-agent-sdk) (1.9.0)\r\nRequirement already satisfied: jiter<1,>=0.4.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from openai>=1.57.0->fabric-data-agent-sdk) (0.9.0)\r\nRequirement already satisfied: pydantic<3,>=1.9.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from openai>=1.57.0->fabric-data-agent-sdk) (2.11.4)\r\nRequirement already satisfied: tqdm>4 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from openai>=1.57.0->fabric-data-agent-sdk) (4.66.4)\r\nRequirement already satisfied: typing-extensions<5,>=4.11 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from openai>=1.57.0->fabric-data-agent-sdk) (4.12.2)\r\nRequirement already satisfied: clr-loader>=0.2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (0.2.7.post0)\r\nRequirement already satisfied: graphviz>=0.20.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (0.20.3)\r\nRequirement already satisfied: pyarrow>=12.0.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (17.0.0)\r\nRequirement already satisfied: pythonnet>=3.0.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (3.0.3)\r\nRequirement already satisfied: scikit-learn>=1.2.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (1.5.0)\r\nRequirement already satisfied: setuptools>=68.2.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (75.8.2)\r\nRequirement already satisfied: rich>=13.3.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (13.9.4)\r\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cffi>=1.17 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from clr-loader>=0.2.5->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (1.17.1)\r\nRequirement already satisfied: numpy>=1.16.6 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from pyarrow>=12.0.1->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (1.26.4)\r\nRequirement already satisfied: annotated-types>=0.6.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.57.0->fabric-data-agent-sdk) (0.7.0)\r\nRequirement already satisfied: pydantic-core==2.33.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.57.0->fabric-data-agent-sdk) (2.33.2)\r\nRequirement already satisfied: typing-inspection>=0.4.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.57.0->fabric-data-agent-sdk) (0.4.0)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from rich>=13.3.5->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (3.0.0)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from rich>=13.3.5->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (2.19.1)\r\nRequirement already satisfied: scipy>=1.6.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (1.15.2)\r\nRequirement already satisfied: joblib>=1.2.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (3.5.0)\r\nRequirement already satisfied: pycparser in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cffi>=1.17->clr-loader>=0.2.5->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (2.22)\r\nRequirement already satisfied: mdurl~=0.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.5->semantic-link-sempy>=0.8.0->fabric-data-agent-sdk) (0.1.2)\r\n"]},{"output_type":"stream","name":"stdout","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"jupyter":{"outputs_hidden":true}},"id":"1681343f-6f69-4bc3-8743-3c248ff27b25"},{"cell_type":"markdown","source":["## Connect to a Data Agent\n","\n","Now that our data is available in the Lakehouse, weâ€™ll create a new **Fabric Data Agent** using the Python SDK.\n","\n","In this step:\n","- We define a name for the agent (e.g., `\"ProductSalesDataAgent\"`)\n","- Use `create_data_agent()` to create a new agent instance\n","- Alternatively, use `FabricDataAgentManagement()` to connect to an existing agent with the same name\n","\n","This agent will be configured to understand your data and respond to natural language questions."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"e6d33a26-823e-410f-bfdb-bb47502344b9"},{"cell_type":"code","source":["from fabric.dataagent.client import (\n","    FabricDataAgentManagement,\n","    create_data_agent,\n","    delete_data_agent,\n",")\n","\n","# Define the name for the Data Agent\n","data_agent_name = \"AdvWorksDataAgent\"\n","\n","# Create a new Data Agent (run this once)\n","data_agent = create_data_agent(data_agent_name)\n","\n","# If the Data Agent already exists, use this instead to connect:\n","# data_agent = FabricDataAgentManagement(data_agent_name)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:13:22.1003675Z","session_start_time":null,"execution_start_time":"2025-05-06T01:13:22.101362Z","execution_finish_time":"2025-05-06T01:13:25.1797557Z","parent_msg_id":"b4fe23c5-93c3-421e-8d74-9aa10738dfff"}},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"1736c52e-c465-4764-b5c9-36320addeed1"},{"cell_type":"markdown","source":["In this step, we configure the Data Agent to work with a **Lakehouse** data source.\n","\n","- We specify the Lakehouse name (e.g., `EvaluationLH`)\n","- Optionally, we register it with the agent if it hasnâ€™t been added yet\n","- We then select specific tables from the `dbo` schema that the agent should use to answer questions\n","\n","These tables will form the structured foundation the agent relies on to generate accurate responses."],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"7531d750-344c-4e84-8564-70b376c66552"},{"cell_type":"code","source":["# Add a Lakehouse as the data source for the agent\n","lakehouse_name = \"EvaluationLH\"\n","\n","# Supported types include: \"lakehouse\", \"kqldatabase\", \"datawarehouse\", or \"semanticmodel\"\n","data_agent.add_datasource(lakehouse_name, type=\"lakehouse\")\n","\n","# Retrieve the data source object (assumes one was added)\n","datasource = data_agent.get_datasources()[0]\n","\n","# Select relevant tables from the Lakehouse (schema: dbo)\n","datasource.select(\"dbo\", \"dimcustomer\")\n","datasource.select(\"dbo\", \"dimdate\")\n","datasource.select(\"dbo\", \"dimgeography\")\n","datasource.select(\"dbo\", \"dimproduct\")\n","datasource.select(\"dbo\", \"dimproductcategory\")\n","datasource.select(\"dbo\", \"dimpromotion\")\n","datasource.select(\"dbo\", \"dimreseller\")\n","datasource.select(\"dbo\", \"dimsalesterritory\")\n","datasource.select(\"dbo\", \"factinternetsales\")\n","datasource.select(\"dbo\", \"factresellersales\")\n","\n","# Publish the data agent\n","data_agent.publish()\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:16:22.8020828Z","session_start_time":null,"execution_start_time":"2025-05-06T01:16:22.8029534Z","execution_finish_time":"2025-05-06T01:16:33.6832767Z","parent_msg_id":"2c4f7ba6-a7d7-4130-9b93-59bf17ce98e5"}},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"ab169252-98e1-4606-8b7a-516c9f2488ce"},{"cell_type":"markdown","source":["## Define ground truth questions and expected answers\n","\n","To evaluate the accuracy of your Data Agent, you'll need a test dataset consisting of natural language questions and their expected answers.\n","\n","In this step:\n","- We define a small set of ground truth examples using a pandas DataFrame\n","- Each row contains a `question` and the `expected_answer`\n","- You can customize these examples based on the data and use cases relevant to your agent\n","\n","Optionally, you can load this dataset from a CSV file if you're working with a larger or pre-curated set of evaluation cases."],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"2411506b-fa48-4f68-a6b5-74bd34bf35ae"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Create DataFrame with \"question,expected_answer\". Please update the questions and expected_answers as per the requirement.\n","df = pd.DataFrame(columns=[\"question\", \"expected_answer\"],\n","                  data=[\n","                    [\"What were our total sales in 2014?\", \"45,694.7\"],\n","                    [\"What is the most sold product?\", \"Mountain-200 Black, 42\"],\n","                    [\"What are the most expensive items that have never been sold?\", \"Road-450 Red, 60\"],\n","                ])\n","\n","# You can also oad from input CSV file with data in format \"question,expected_answer\"\n","# input_file_path = \"/lakehouse/default/Files/Data/Input/groundtruth.csv\"\n","# df = pd.read_csv(input_file_path)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:16:39.4693468Z","session_start_time":null,"execution_start_time":"2025-05-06T01:16:39.4702921Z","execution_finish_time":"2025-05-06T01:16:39.8148959Z","parent_msg_id":"35f77408-fca6-4716-a035-df60d3ff339d"}},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"17b2a29f-6be6-49f9-b7c0-68f71ca11846"},{"cell_type":"markdown","source":["## Configure Evaluation Parameters\n","\n","Before running the evaluation, we define a few optional parameters to control where and how results are stored:\n","\n","- `workspace_name`: (Optional) Use this if your Data Agent is located in a different workspace.\n","- `table_name`: The base name of the output table where evaluation results will be stored. This will generate:\n","  - `<table_name>`: A summary of the evaluation results.\n","  - `<table_name>_steps`: A detailed log of reasoning steps for each question.\n","- `data_agent_stage`: Set to `\"sandbox\"` or `\"production\"` depending on which version of the agent you want to evaluate.\n","\n","These settings help you organize and retrieve evaluation outputs from your Lakehouse environment.\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"f738d8aa-e598-4bb0-94ff-35cd93b47bb6"},{"cell_type":"code","source":["from fabric.dataagent.evaluation import evaluate_data_agent\n","\n","\n","# Workspace Name (Optional) if Data Agent is in different workspace\n","workspace_name = None\n","\n","# Table name (Optional) to store the evaluation result. Default value is 'evaluation_output'\n","# After evaluation there will be two tables one with provided <table_name> for evaluation output and other with <table_name>_steps for detailed steps.\n","table_name = \"demo_evaluation_output\"\n","\n","# Data Agent stage ie., sandbox or production. Default to production.\n","data_agent_stage = \"sandbox\"\n","\n","# Evaluation output table name\n","table_name = \"demo_evaluation_output\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:16:39.603202Z","session_start_time":null,"execution_start_time":"2025-05-06T01:16:39.8161334Z","execution_finish_time":"2025-05-06T01:16:40.1644658Z","parent_msg_id":"cc843c39-c72f-40cd-beef-a3e36356c42b"}},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"2748d60b-400f-48f1-81ee-7c6c57ae6b15"},{"cell_type":"markdown","source":["## Run the Evaluation\n","\n","Now we're ready to evaluate the Data Agent using the ground truth dataset we defined earlier.\n","\n","The `evaluate_data_agent()` function will:\n","- Run each question against the Data Agent\n","- Compare the actual response to the expected answer\n","- Log results and reasoning steps to the specified Lakehouse tables\n","\n","It returns a unique `evaluation_id` which you can use to retrieve summaries or detailed results later.\n","\n","Let's run the evaluation and capture the ID for this run.\n"],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"323e51aa-23fe-4fce-9126-38ee1751b7a1"},{"cell_type":"code","source":["# Evaluate the Data Agent. Returns the unique id for the evaluation run\n","evaluation_id = evaluate_data_agent(df, data_agent_name, workspace_name=workspace_name, table_name=table_name, data_agent_stage=data_agent_stage)\n","\n","print(f\"Unique Id for the current evaluation run: {evaluation_id}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:16:39.8330748Z","session_start_time":null,"execution_start_time":"2025-05-06T01:16:40.1656522Z","execution_finish_time":"2025-05-06T01:17:46.9371031Z","parent_msg_id":"131e0a2a-b6ac-403a-8149-72b430000c18"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows:   0%|          | 0/3 [00:00<?, ?step/s]"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:19<00:39, 19.76s/step]"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:39<00:19, 19.95s/step]"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:01<00:00, 20.52s/step]\rProcessing Rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:01<00:00, 20.35s/step]\n"]},{"output_type":"stream","name":"stdout","text":["Unique Id for the current evaluation run: 4418ac7d-47b6-41fc-afa8-8d47ee62dd12\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"87fdf8c0-6b6d-4b41-b393-4001663bd2e5"},{"cell_type":"markdown","source":["## View evaluation summary\n","\n","After the evaluation run completes, you can retrieve a high-level summary using the `get_evaluation_summary()` function.\n","\n","This summary includes:\n","- Total number of questions evaluated\n","- Counts of correct, incorrect, and unclear responses\n","- Overall accuracy metrics\n","\n","Use this step to quickly assess how well your Data Agent performed.\n"],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"65dda6ba-73b9-4264-bc63-f0645e639cba"},{"cell_type":"code","source":["# Import the function to retrieve evaluation summaries\n","from fabric.dataagent.evaluation import get_evaluation_summary\n","\n","# Retrieve the summary of the evaluation results using the specified table name\n","# This returns a DataFrame with aggregated metrics like counts of true/false/unclear responses\n","eval_summary_df = get_evaluation_summary(table_name)\n","\n","eval_summary_df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:18:24.9822351Z","session_start_time":null,"execution_start_time":"2025-05-06T01:18:50.2730737Z","execution_finish_time":"2025-05-06T01:18:50.6291227Z","parent_msg_id":"3f7f015f-c64a-4ac2-aaec-fb2111a590f0"}},"metadata":{}},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"index                         evaluation_id  true_count  false_count  \\\n0      43cc3b51-33df-4efe-ae07-2f2fd5d5f2df           2            1   \n1      4418ac7d-47b6-41fc-afa8-8d47ee62dd12           2            1   \n\nindex  unclear_count  true_percentage  \n0                  0        66.666667  \n1                  0        66.666667  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>index</th>\n      <th>evaluation_id</th>\n      <th>true_count</th>\n      <th>false_count</th>\n      <th>unclear_count</th>\n      <th>true_percentage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>43cc3b51-33df-4efe-ae07-2f2fd5d5f2df</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>66.666667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4418ac7d-47b6-41fc-afa8-8d47ee62dd12</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>66.666667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a851e940-41d4-4308-8145-c571854c5bb4"},{"cell_type":"markdown","source":["## Retrieve Detailed Evaluation Results\n","\n","To analyze the agent's performance question-by-question, use the `get_evaluation_details()` function.\n","\n","This provides a detailed view of:\n","- The original question\n","- The expected answer\n","- The agent's actual response\n","- The evaluation outcome (`true`, `false`, or `unclear`)\n","- A link to the Fabric thread (accessible only to the evaluator)\n","\n","You can also control:\n","- `get_all_rows`: Set to `True` to return both successful and failed evaluations (defaults to `False`, which returns only failed cases).\n","- `verbose`: Set to `True` to print a summary alongside the DataFrame.\n","\n","This is especially useful for debugging incorrect responses and improving your agent's accuracy over time.\n"],"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"nteract":{"transient":{"deleting":false}}},"id":"4be6eb00-9de4-4f24-a66e-f5989290da7d"},{"cell_type":"code","source":["# Import the function to retrieve detailed evaluation results\n","from fabric.dataagent.evaluation import get_evaluation_details\n","\n","# Unique identifier for the evaluation run (already captured earlier)\n","# You can hardcode an ID here if needed\n","# evaluation_id = 'd36ce205-a88d-42bd-927d-260ec2e2a479'\n","\n","# Whether to return all evaluation results (True) or only failed ones (False, default)\n","get_all_rows = True\n","\n","# Whether to print a summary of the evaluation results to the console (optional)\n","verbose = True\n","\n","# Fetch detailed evaluation results as a DataFrame\n","# This includes question, expected answer, actual answer, evaluation status, and diagnostic info\n","eval_details_df = get_evaluation_details(\n","    evaluation_id,\n","    table_name,\n","    get_all_rows=get_all_rows,\n","    verbose=verbose\n",")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:16:40.3629965Z","session_start_time":null,"execution_start_time":"2025-05-06T01:17:47.3311705Z","execution_finish_time":"2025-05-06T01:17:47.764882Z","parent_msg_id":"c6c4d2a0-472b-47a0-bc50-4def246ca357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><div style=\"text-align: center !important;\">question</div></th>\n      <th><div style=\"text-align: center !important;\">expected_answer</div></th>\n      <th><div style=\"text-align: center !important;\">evaluation_judgement</div></th>\n      <th><div style=\"text-align: center !important;\">actual_answer</div></th>\n      <th><div style=\"text-align: center !important;\">thread_url</div></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><div style=\"text-align: left !important;\">What were our total sales in 2014?</div></td>\n      <td><div style=\"text-align: left !important;\">45,694.7</div></td>\n      <td><div style=\"text-align: left !important;\">True</div></td>\n      <td><div style=\"text-align: left !important;\"><pre>The total sales in 2014 were 45,694.7.</pre></div></td>\n      <td><div style=\"text-align: left !important;\"><a href=\"https://msit.fabric.microsoft.com/workloads/de-ds/dataagents/5f0d0228-2be2-468c-9926-9aae27079025/externalThread?debug.aiSkillThreadIdOverride=thread_sIbAXYDgHlXoRvOAp61BecGo&debug.aiSkillViewPublishedOverride=0\" target=\"_blank\">thread_sIbAXYDgHlXoRvOAp61BecGo</a></div></td>\n    </tr>\n    <tr>\n      <td><div style=\"text-align: left !important;\">What is the most sold product?</div></td>\n      <td><div style=\"text-align: left !important;\">Mountain-200 Black, 42</div></td>\n      <td><div style=\"text-align: left !important;\">False</div></td>\n      <td><div style=\"text-align: left !important;\"><pre>The most sold product is the \"Water Bottle - 30 oz.\"</pre></div></td>\n      <td><div style=\"text-align: left !important;\"><a href=\"https://msit.fabric.microsoft.com/workloads/de-ds/dataagents/5f0d0228-2be2-468c-9926-9aae27079025/externalThread?debug.aiSkillThreadIdOverride=thread_GJTABYSGsBchRW4qVZnbuaSF&debug.aiSkillViewPublishedOverride=0\" target=\"_blank\">thread_GJTABYSGsBchRW4qVZnbuaSF</a></div></td>\n    </tr>\n    <tr>\n      <td><div style=\"text-align: left !important;\">What are the most expensive items that have never been sold?</div></td>\n      <td><div style=\"text-align: left !important;\">Road-450 Red, 60</div></td>\n      <td><div style=\"text-align: left !important;\">True</div></td>\n      <td><div style=\"text-align: left !important;\"><pre>The most expensive item that has never been sold is the \"Road-450 Red, 60\" with a list price of $1,457.99.</pre></div></td>\n      <td><div style=\"text-align: left !important;\"><a href=\"https://msit.fabric.microsoft.com/workloads/de-ds/dataagents/5f0d0228-2be2-468c-9926-9aae27079025/externalThread?debug.aiSkillThreadIdOverride=thread_r7Faa2vzm4zoSRLDLTld5RpV&debug.aiSkillViewPublishedOverride=0\" target=\"_blank\">thread_r7Faa2vzm4zoSRLDLTld5RpV</a></div></td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"2268bc2d-3213-440a-9193-10fbcc514a4c"},{"cell_type":"markdown","source":["## Use a custom prompt to evaluate agent responses\n","\n","In some cases, simple string matching may not be sufficient to determine if the agent's response is correctâ€”especially when responses vary in format but are semantically equivalent.\n","\n","You can define a **custom critic prompt** using the `critic_prompt` parameter in `evaluate_data_agent()`. This prompt will be used by an LLM to decide whether the actual answer is equivalent to the expected answer.\n","\n","The prompt must include the following placeholders:\n","- `{query}`: The original user question\n","- `{expected_answer}`: The expected result\n","- `{actual_answer}`: The agent's generated response\n","\n","Once the evaluation is complete, you can retrieve the summary results using `get_evaluation_summary()` and track the run using the printed `evaluation_id`.\n","\n","This method gives you more flexibility in how you assess correctness, especially for complex or domain-specific outputs.\n"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"616c1f32-a6cc-4d9b-ab62-32e7665c6507"},{"cell_type":"code","source":["from fabric.dataagent.evaluation import evaluate_data_agent\n","\n","# Define a custom prompt to evaluate whether the agent's actual response matches the expected answer.\n","# The prompt should include placeholders: {query}, {expected_answer}, and {actual_answer}\n","critic_prompt = \"\"\"\n","        Given the following query, expected answer, and actual answer, please determine if the actual answer is equivalent to expected answer. If they are equivalent, respond with 'yes'.\n","\n","        Query: {query}\n","\n","        Expected Answer:\n","        {expected_answer}\n","\n","        Actual Answer:\n","        {actual_answer}\n","\n","        Is the actual answer equivalent to the expected answer?\n","        \"\"\"\n","\n","# Evaluate the Data Agent using the custom critic prompt\n","# Returns a unique evaluation ID for tracking and analysis\n","evaluation_id_critic = evaluate_data_agent(\n","    df,\n","    data_agent_name,\n","    critic_prompt=critic_prompt,\n","    table_name=table_name,\n","    data_agent_stage=\"sandbox\"\n",")\n","\n","# Retrieve the summary of this evaluation run\n","eval_summary_df_critic = get_evaluation_summary(table_name)\n","\n","# Display the unique ID for reference\n","print(f\"Unique Id for the current evaluation run: {evaluation_id_critic}\")\n","\n","eval_summary_df_critic"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"ba77350c-9e49-4dd3-9079-7cafe4f9c5b5","normalized_state":"finished","queued_time":"2025-05-06T01:16:40.8163683Z","session_start_time":null,"execution_start_time":"2025-05-06T01:17:47.7658555Z","execution_finish_time":"2025-05-06T01:18:50.2718534Z","parent_msg_id":"40040e00-a38d-4002-ba49-42cb4c108b8a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows:   0%|          | 0/3 [00:00<?, ?step/s]"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:19<00:39, 19.84s/step]"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:39<00:19, 19.47s/step]"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:58<00:00, 19.27s/step]\rProcessing Rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:58<00:00, 19.36s/step]\n"]},{"output_type":"stream","name":"stdout","text":["Unique Id for the current evaluation run: 43cc3b51-33df-4efe-ae07-2f2fd5d5f2df\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e81ca94e-45db-4421-852b-8d89da04dcb2"},{"cell_type":"markdown","source":["## "],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"82e8792c-ec0c-4d80-9e3f-ecd1e8ea09c7"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"display_name":"synapse_pyspark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"68491dd8-8ec1-4591-9874-29723aa54111","default_lakehouse_name":"EvaluationLH","default_lakehouse_workspace_id":"9524e38b-5c6e-4838-adb3-500b0a93df45","known_lakehouses":[{"id":"68491dd8-8ec1-4591-9874-29723aa54111"}]}}},"nbformat":4,"nbformat_minor":5}