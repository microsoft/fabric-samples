{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4afa8c-259d-4538-a1ef-3911a86ff0fd",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 3: EDA and Data Prep\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the initial phase of data analysis, focusing on understanding data characteristics, patterns, and relationships through visualizations and summaries. Data Preparation in machine learning involves preprocessing raw data to ensure its suitability for model training, including tasks like handling missing values, encoding variables, scaling features, and splitting data for training and testing. Both EDA and Data Preparation are crucial stages for extracting insights and preparing data for machine learning models.\n",
    "\n",
    "### Exercise overview\n",
    "\n",
    "In this exercise, you'll explore the dataset through EDA, uncovering its patterns and relationships. You'll also perform tasks like data preprocessing, handling missing values, and feature engineering to make the data ready for machine learning training.\n",
    "\n",
    "### Helpful links\n",
    "- [Data Wrangler in Microsoft Fabric](https://aka.ms/fabric/datawrangler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0a497-4ef8-4149-ac0a-5311b65099b3",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Pre-Requisite\n",
    "\n",
    "For this Exercise, we expect that you have completed and ran **Lab 2: Access your data through semantic link**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e38c81-acc7-4c4b-8d68-56779c9abfa9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Setup your notebook\n",
    "\n",
    "### Select Lakehouse (FC_Workshop)\n",
    "First, add the Lakehouse you created from the prior lab exercise.\n",
    "\n",
    "<br>\n",
    "\n",
    "![image-alt-text](https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/add-lakehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bde78-08ff-49a4-9592-7c036d2408a0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Load the data as a Spark DataFrame\n",
    "\n",
    "Load the data that you have created and saved in the lakehouse from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cf156-d22a-4a88-ab7d-d9c30fa924e3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM FC_Workshop.churnfromsemanticlink\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce4bed-8d9f-45fc-b8da-fcacc852d7c8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Convert the spark DataFrame to pandas DataFrame for easier processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323b650-4bd0-4dfb-8642-4754aaf2d1d6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5557ae8-cd0d-41cc-8472-8a50a9d9561c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset contains churn status of 10,000 customers along with 13 attributes that include last credit score, geographical location (Germany, France, Spain), gender (male, female), age, tenure (years of being bank's customer), account balance, estimated salary, number of products that a customer has purchased through the bank, credit card status (whether a customer has a credit card or not), and active member status (whether an active bank's customer or not).\n",
    "\n",
    "The dataset also includes columns such as account ID, customer ID, and customer surname that should have no impact on customer's decision to leave the bank. The event that defines the customer's churn is the closing of the customer's bank account, therefore, the column `Exited` in the dataset refers to customer's abandonment. Since you don't have much context about these attributes, you'll proceed without having background information about the dataset. Your aim is to understand how these attributes contribute to the `Exited` status.\n",
    "\n",
    "Out of the 10,000 customers, only 2037 customers (around 20%) have left the bank. Therefore, given the class imbalance ratio, it is recommended to generate synthetic data.\n",
    "\n",
    "- churn.csv\n",
    "\n",
    "|\"CustomerId\"|\"Surname\"|\"Last Credit Score\"|\"Geography\"|\"Gender\"|\"Age\"|\"Tenure\"|\"Balance\"|\"Number Of Products\"|\"Has Credit Card\"|\"Is Active\"|\"EstimatedSalary\"|\"Exited\"|\"AccountId\"|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|15634602|Hargrave|619|France|Female|42|2|0.00|1|1|1|101348.88|True|4602|\n",
    "|15647311|Hill|608|Spain|Female|41|1|83807.86|1|0|1|112542.58|False|8401|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811b929e-1897-43b9-a838-b6ac866bb5b5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Perform Exploratory Data Analysis\n",
    "\n",
    "Explore the raw data with `display`, do some basic statistics and show chart views. You first need to import required libraries for data visualization such as `seaborn` which is a Python data visualization library to provide a high-level interface for building visuals on dataframes and arrays. Learn more about [`seaborn`](https://seaborn.pydata.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5f7ae-8c41-43c5-8df9-a52c6855582e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)})\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import rc, rcParams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9ffb8-bae3-495b-bd4a-437c0dfa302c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Display raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d45e85-d53e-48e6-9de3-4d04092a06d4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1a762-7b0b-4857-adfb-832a4697b7fb",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Use Data Wrangler to perform initial data cleansing\n",
    "\n",
    "To explore and transform any pandas Dataframes in your notebook, launch Data Wrangler directly from the notebook.\n",
    "\n",
    "1. Under the notebook ribbon Data tab, select Launch Data Wrangler. You'll see a list of activated pandas DataFrames available for editing.\n",
    "2. Select the DataFrame you wish to open in Data Wrangler. Since this notebook only contains one DataFrame at this point, select `df`.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://sdkstorerta.blob.core.windows.net/churnblob/select_datawrangler.png\"  width=\"40%\" height=\"10%\" title=\"Screenshot shows where to access the Data Wrangler.\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Data Wrangler launches and generates a descriptive overview of your data. The table in the middle shows each data column. The Summary panel next to the table shows information about the DataFrame. When you select a column in the table, the summary updates with information about the selected column. In some instances, the data displayed and summarized will be a truncated view of your DataFrame. When this happens, you'll see warning image in the summary pane. Hover over this warning to view text explaining the situation.\n",
    "\n",
    "Each operation you do can be applied in a matter of clicks, updating the data display in real time and generating code that you can save back to your notebook as a reusable function.\n",
    "\n",
    "The rest of this section walks you through the steps to perform data cleaning with Data Wrangler.\n",
    "\n",
    "##### Drop duplicate rows\n",
    "\n",
    "On the left panel is a list of operations (such as `Find and replace`, `Format`, `Formulas`, `Numeric`) you can perform on the dataset.\n",
    "\n",
    "1. Expand `Find and replace` and select `Drop duplicate rows`.\n",
    "\n",
    "2. A panel appears for you to select the list of columns you want to compare to define a duplicate row. Select `CustomerId`, and `AccountId`.\n",
    "\n",
    "In the middle panel is a preview of the results of this operation. Under the preview is the code to perform the operation. In this instance, the data appears to be unchanged. But since you're looking at a truncated view, it's a good idea to still apply the operation.\n",
    "\n",
    "3. Select `Apply` (either at the side or at the bottom) to go to the next step.\n",
    "\n",
    "##### Drop rows with missing data\n",
    "\n",
    "Use Data Wrangler to drop rows with missing data across all columns.\n",
    "\n",
    "1. Select `Drop missing values` from `Find and replace`.\n",
    "\n",
    "2. Choose `Select all` from the `Target columns`.\n",
    "\n",
    "3. Select `Apply` to go on to the next step.\n",
    "\n",
    "##### Drop columns\n",
    "Use Data Wrangler to drop columns that you don't need.\n",
    "\n",
    "1. Expand `Schema` and select `Drop columns`.\n",
    "\n",
    "2. Select `CustomerId`, `Surname`, and `AccountId`. These columns appear in red in the preview, to show they're changed by the code (in this case, dropped.)\n",
    "\n",
    "3.Select `Apply` to go on to the next step.\n",
    "\n",
    "##### Add code to notebook\n",
    "\n",
    "Each time you select `Apply`, a new step is created in the `Cleaning steps` panel on the bottom left. At the bottom of the panel, select `Preview code for all steps` to view a combination of all the separate steps.\n",
    "\n",
    "Select `Add code to notebook` at the top left to close Data Wrangler and add the code automatically. The `Add code to notebook` wraps the code in a function, then calls the function.\n",
    "\n",
    "\n",
    ">[!NOTE]\n",
    ">Data Wrangler can not be opened while the notebook kernel is busy. The cell execution must complete prior to launching Data Wrangler.\n",
    "\n",
    ">[!NOTE]\n",
    ">The code generated by Data Wrangler won't be applied until you manually run the new cell in the notebook.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/menu_datawrangler.png\"  width=\"45%\" height=\"10%\" title=\"Screenshot shows Data Wrangler menu.\"> \n",
    "<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/missing_data_datawrangler.png\"  width=\"45%\" height=\"10%\" title=\"Screenshot shows Data Wrangler missing data display.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248f9a5-4cfa-4f67-9324-2d9acc15efd6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### <span style=\"color:red;\"> Exercise 1 </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49be7d-f4a7-4472-b9a7-fd964511aa8a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Add the code generated by Data Wrangler from the above data cleaning steps\n",
    "# TODO: Modification is to add inplace=True to each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5ffe6-4000-4aed-a6c5-32d308915c8a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78408d-bc99-4ed7-996a-bad394a84499",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Unlocking Insights with EDA and Visualization\n",
    "\n",
    "##### Determine attributes\n",
    "\n",
    "Use this code to determine categorical, numerical, and target attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685951ba-f18e-48cd-965f-61e493aa8fce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Determine the dependent (target) attribute\n",
    "dependent_variable_name = \"Exited\"\n",
    "print(dependent_variable_name)\n",
    "# Determine the categorical attributes\n",
    "categorical_variables = [col for col in df_clean.columns if col in \"O\"\n",
    "                        or df_clean[col].nunique() <=5\n",
    "                        and col not in \"Exited\"]\n",
    "print(categorical_variables)\n",
    "# Determine the numerical attributes\n",
    "numeric_variables = [col for col in df_clean.columns if df_clean[col].dtype != \"object\"\n",
    "                        and df_clean[col].nunique() >5]\n",
    "print(numeric_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f171810-37f3-46e1-9c96-4f7cd0422053",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "##### The five-number summary \n",
    "\n",
    "Show the five-number summary (the minimum score, first quartile, median, third quartile, the maximum score) for the numerical attributes, using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba26dc-163e-4fe9-a075-3c4650d019b4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_num_cols = df_clean[numeric_variables]\n",
    "sns.set(font_scale = 0.7) \n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 3, gridspec_kw =  dict(hspace=0.3), figsize = (17,8))\n",
    "fig.tight_layout()\n",
    "for ax,col in zip(axes.flatten(), df_num_cols.columns):\n",
    "    sns.boxplot(x = df_num_cols[col], color='green', ax = ax)\n",
    "# fig.suptitle('visualize and compare the distribution and central tendency of numerical attributes', color = 'k', fontsize = 12)\n",
    "fig.delaxes(axes[1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243abf4-4e9d-429a-bc6d-f362ba04208c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Distribution of exited and non-exited customers \n",
    "\n",
    "Show the distribution of exited versus non-exited customers across the categorical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e92a21-f3a3-4b4b-b758-651e230d22a7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "attr_list = ['Geography', 'Gender', 'Has Credit Card', 'Is Active', 'Number Of Products', 'Tenure']\n",
    "fig, axarr = plt.subplots(2, 3, figsize=(15, 4))\n",
    "for ind, item in enumerate (attr_list):\n",
    "    sns.countplot(x = item, hue = 'Exited', data = df_clean, ax = axarr[ind%2][ind//2])\n",
    "fig.subplots_adjust(hspace=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e34f7e9-4ec8-465c-b6ba-70c6e13234aa",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Distribution of numerical attributes\n",
    "\n",
    "Show the the frequency distribution of numerical attributes using histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168302f-a3e3-4d7f-8946-5d3b73f11187",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "columns = df_num_cols.columns[: len(df_num_cols.columns)]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18, 8)\n",
    "length = len(columns)\n",
    "for i,j in itertools.zip_longest(columns, range(length)):\n",
    "    plt.subplot((length // 2), 3, j+1)\n",
    "    plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n",
    "    df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n",
    "    plt.title(i)\n",
    "# fig = fig.suptitle('distribution of numerical attributes', color = 'r' ,fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9d805-dbe6-4b21-9973-30dc785b6f47",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Summary of observations from the exploratory data analysis\n",
    "\n",
    "- Most of the customers are from France comparing to Spain and Germany, while Spain has the lower churn rate comparing to France and Germany.\n",
    "- Most of the customers have credit cards.\n",
    "- There are customers whose age and credit score are above 60 and below 400, respectively, but they can't be considered as outliers.\n",
    "- Very few customers have more than two of the bank's products.\n",
    "- Customers who aren't active have a higher churn rate.\n",
    "- Gender and tenure years don't seem to have an impact on customer's decision to close the bank account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd880334-c8a9-4acb-845c-be44c7808a4a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Perform feature engineering \n",
    "\n",
    "The following feature engineering generates new attributes based on current attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575737d0-1baf-47b5-8b5d-7172a7e115ca",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_clean[\"NewTenure\"] = df_clean[\"Tenure\"]/df_clean[\"Age\"]\n",
    "df_clean[\"NewCreditsScore\"] = pd.qcut(df_clean['Last Credit Score'], 6, labels = [1, 2, 3, 4, 5, 6])\n",
    "df_clean[\"NewAgeScore\"] = pd.qcut(df_clean['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "df_clean[\"NewBalanceScore\"] = pd.qcut(df_clean['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\n",
    "df_clean[\"NewEstSalaryScore\"] = pd.qcut(df_clean['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "df_clean['Exited'] = df['Exited'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1085412-113d-4fc0-a792-454705c5363a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Use Data Wrangler to perform one-hot encoding\n",
    "\n",
    "Data Wrangler can also be used to perform one-hot encoding. To do so, re-open Data Wrangler. This time, select the `df_clean` data.\n",
    "\n",
    "1. Expand `Formulas` and select `One-hot encode`.\n",
    "2. A panel appears for you to select the list the column you want to perform one-hot encoding on. Select `Geography`.\n",
    "\n",
    "You should be able to see in the preview that column `Geography` is dropped, while three new columns `Geography_Germany`, `Geography_France`, and `Geography_Spain` are added as a result of one-hot encoding.\n",
    "\n",
    "1. Expand `Schema` and select `Change column type`.\n",
    "2. A panel appears for you to select the list of columns you want to change their type as target columns. Select `Geography_Germany`, `Geography_France`, and `Geography_Spain` as the newly added columns to the DataFarme.\n",
    "3. Select the `New type` as `float64`.\n",
    "3. Select `Apply` (either at the side or at the bottom) to go to the next step.\n",
    "\n",
    "You could copy the generated code, close Data Wrangler to return to the notebook, then paste into a new cell. Or, select `Add code to notebook` at the top left to close Data Wrangler and add the code automatically.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/1hotencoding_data_wrangler.png\"  width=\"45%\" height=\"20%\" title=\"Screenshot shows one-hot encoding in the Data Wrangler\"> \n",
    "<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/1hotencoding_selectcolumns_data_wrangler.png\"  width=\"45%\" height=\"20%\" title=\"Screenshot shows selection of columns in the Data Wrangler.\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef856b81-4bfb-4bb3-ae6e-0a9cc9036b77",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Code generated by Data Wrangler for pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df_clean):\n",
    "    # One-hot encode column: 'Geography'\n",
    "    insert_loc = df_clean.columns.get_loc('Geography')\n",
    "    df_clean = pd.concat([df_clean.iloc[:,:insert_loc], pd.get_dummies(df_clean.loc[:, ['Geography']]), df_clean.iloc[:,insert_loc+1:]], axis=1)\n",
    "    # Change column type to float64 for columns: 'Geography_France', 'Geography_Germany', 'Geography_Spain'\n",
    "    df_clean = df_clean.astype({'Geography_France': 'float64', 'Geography_Germany': 'float64', 'Geography_Spain': 'float64'})\n",
    "    return df_clean\n",
    "\n",
    "df_clean_1 = clean_data(df_clean.copy())\n",
    "df_clean_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e52d2-05e4-423d-ac38-03aa9e80caa7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You can repeat the above steps to perform one-hot encoding on other columns, e.g., `Gender`, `Had Credit Card`, and `Is Active`. In the following, the code generated by the Data Wrangler for each of the columns is added to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f9511-8196-4476-8802-5bf2ca8362b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Code generated by Data Wrangler for pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df_clean_1):\n",
    "    # One-hot encode column: 'Gender'\n",
    "    insert_loc = df_clean_1.columns.get_loc('Gender')\n",
    "    df_clean_1 = pd.concat([df_clean_1.iloc[:,:insert_loc], pd.get_dummies(df_clean_1.loc[:, ['Gender']]), df_clean_1.iloc[:,insert_loc+1:]], axis=1)\n",
    "    # Change column type to float64 for columns: 'Gender_Female', 'Gender_Male'\n",
    "    df_clean_1 = df_clean_1.astype({'Gender_Female': 'float64', 'Gender_Male': 'float64'})\n",
    "    return df_clean_1\n",
    "\n",
    "df_clean_2 = clean_data(df_clean_1.copy())\n",
    "df_clean_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8b6d5-a097-4589-bb3f-2a47133971bd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Code generated by Data Wrangler for pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df_clean_2):\n",
    "    # One-hot encode column: 'Has Credit Card'\n",
    "    insert_loc = df_clean_2.columns.get_loc('Has Credit Card')\n",
    "    df_clean_2 = pd.concat([df_clean_2.iloc[:,:insert_loc], pd.get_dummies(df_clean_2.loc[:, ['Has Credit Card']]), df_clean_2.iloc[:,insert_loc+1:]], axis=1)\n",
    "    # Change column type to float64 for columns: 'Has Credit Card_No', 'Has Credit Card_Yes'\n",
    "    df_clean_2 = df_clean_2.astype({'Has Credit Card_No': 'float64', 'Has Credit Card_Yes': 'float64'})\n",
    "    return df_clean_2\n",
    "\n",
    "df_clean_3 = clean_data(df_clean_2.copy())\n",
    "df_clean_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de61991-5196-4b80-b842-66d9a8547386",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Code generated by Data Wrangler for pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df_clean_3):\n",
    "    # One-hot encode column: 'Is Active'\n",
    "    insert_loc = df_clean_3.columns.get_loc('Is Active')\n",
    "    df_clean_3 = pd.concat([df_clean_3.iloc[:,:insert_loc], pd.get_dummies(df_clean_3.loc[:, ['Is Active']]), df_clean_3.iloc[:,insert_loc+1:]], axis=1)\n",
    "    # Change column type to float64 for columns: 'Is Active_No', 'Is Active_Yes'\n",
    "    df_clean_3 = df_clean_3.astype({'Is Active_No': 'float64', 'Is Active_Yes': 'float64'})\n",
    "    return df_clean_3\n",
    "\n",
    "df_clean_4 = clean_data(df_clean_3.copy())\n",
    "df_clean_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d04226-55f3-4a9d-961d-207dab93352c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 6: Rename Columns\n",
    "\n",
    "In the provided code snippet, a dictionary named `column_mapping` is defined to map existing column names in a DataFrame, which contain white spaces and underscores, to new, more concise column names without white spaces. This mapping is then used to rename the columns of a DataFrame named `df_clean_4` using its `rename` method. The result of this operation is stored in a new DataFrame called `df_clean_renamed`, where the column names are updated according to the mappings specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08847543-26cd-4d49-a642-e578325bdfe1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns to avoid white space\n",
    "# Create a dictionary to map old column names to new column names\n",
    "column_mapping = {\n",
    "    \"Has Credit Card_No\": \"HasCreditCard\",\n",
    "    \"Has Credit Card_Yes\": \"HasCreditCard_Yes\",\n",
    "    \"Is Active_No\": \"IsActive_No\",\n",
    "    \"Is Active_Yes\": \"IsActive_Yes\",\n",
    "    \"Number Of Products\": \"NumberOfProducts\",\n",
    "    \"Last Credit Score\": \"LastCreditScore\",\n",
    "}\n",
    "# Rename columns using the rename method\n",
    "df_clean_renamed = df_clean_4.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea0f37-108a-4781-b634-6c0b1a4e893e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 7: Save final data to lakehouse\n",
    "Finally, we will save the cleaned data to the lakehouse to be used for next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f2b4f-98e6-4695-b564-eda838ae920c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"churn_data_clean\"\n",
    "\n",
    "# Create PySpark DataFrame from Pandas\n",
    "sparkDF=spark.createDataFrame(df_clean_renamed) \n",
    "sparkDF.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n",
    "print(f\"Spark dataframe saved to delta table: {table_name}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
