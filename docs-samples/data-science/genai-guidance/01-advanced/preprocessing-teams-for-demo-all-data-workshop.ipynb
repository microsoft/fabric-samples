{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920c6ac4-a767-41ea-a9b4-7666c1b36f03",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Notebook for generating FAQ with Hierarchcal Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da198672-d930-46a7-9fe8-33753122deae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048c284-d32b-42c3-ab20-ac0ff95d19d6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, explode, collect_list, struct, udf, concat, monotonically_increasing_id, to_json, when, from_json, concat_ws, current_timestamp, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType, FloatType\n",
    "from bs4 import BeautifulSoup\n",
    "from synapse.ml.featurize.text import PageSplitter\n",
    "from abc import ABC, abstractmethod \n",
    "import openai\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import uuid \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f9736-7eb3-43de-9791-1156ca6544f6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9001ea4-38de-4445-b51e-ad38d2134803",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# SAS URL of the blob with JSON file\n",
    "sas_url = \"https://aka.ms/funwithteams\"\n",
    "\n",
    "# Download the blob content using the SAS URL\n",
    "response = requests.get(sas_url)\n",
    "blob_content = response.content\n",
    "\n",
    "# Parse the JSON content\n",
    "try:\n",
    "    json_content = json.loads(blob_content)\n",
    "    print(\"JSON is valid.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Invalid JSON: {e}\")\n",
    "    json_content = None\n",
    "\n",
    "# Print the first 20 lines of the JSON content if valid\n",
    "if json_content:\n",
    "    json_str = json.dumps(json_content, indent=4)\n",
    "    json_lines = json_str.split('\\n')\n",
    "    for line in json_lines[:20]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09d56c-84b0-40a7-946c-799c844f33b3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f0a22-1dce-4782-9fd7-4c81c16d4f0e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the JSON data into a flat table\n",
    "df = pd.json_normalize(json_content)\n",
    "\n",
    "# Print the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b01818-e181-4565-836e-9e07dc49f80f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extract conversational texts from Teams Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e70834-61c1-4139-bfb8-ddaaa8e4f8d1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract 'body->content' from JSON string\n",
    "def extract_body_content(json_str):\n",
    "    try:\n",
    "        #json_data = json.loads(str(json_str))\n",
    "        #return json_data['body']['content']\n",
    "        return json_str[0][\"body\"][\"content\"]\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# Apply the function to create a new column 'parent_post_text'\n",
    "df['parent_post_text'] = df['Thread.parent.value'].apply(extract_body_content)\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e0c4f4-d9a7-4ff0-8c33-a4dc12c07ed1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Example of a message that contains a tag of a person's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6a999-d20e-4936-b458-a4ee8eab48c1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Display full content of parent_post_text in row with id = messages_Chemistry_20250320_055839_07f3eb8e\n",
    "full_content = df.loc[df['Id'] == 'messages_Chemistry_20250320_055839_07f3eb8e', 'parent_post_text'].values\n",
    "print(full_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43593f9-3f78-4512-88fe-286180baa817",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Segment parent posts into refined questions with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04831372-e8d4-445d-a256-987ce39e586c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Define the function to segment post into questions\n",
    "def segment_post_into_questions(parent_post_text):\n",
    "    prompt = \"\"\"\n",
    "    You are given a message thread of someone addressing a team with one or more questions. Take it, remove individual names and other \"fluff\" and \n",
    "    break it down into concise self-contained questions, presented in json form of the following structure: \n",
    "    [{\"question\": \"How to synthesize a protein with specific properties?\"}].\n",
    "    If the message contains no questions, return empty JSon array, without comments.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    content = \"Message: \"\n",
    "    if parent_post_text is not None:\n",
    "        content += parent_post_text + \"\\n\\n\"\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    \n",
    "    attempts = 0\n",
    "    result = ''\n",
    "    \n",
    "    print(f\"start to work on input: {parent_post_text}\")\n",
    "    while attempts < 10 and result == '':\n",
    "        try:\n",
    "            attempts+=1 \n",
    "\n",
    "            response = openai.ChatCompletion.create(\n",
    "                #deployment_id='gpt-35-turbo-0125', # see the note in the cell below for an alternative deployment_id.\n",
    "                deployment_id=\"gpt-4-32k\",\n",
    "                messages= messages,\n",
    "                temperature=0,\n",
    "            ) \n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            #print(result)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"sleeping, attempt {attempts}\")\n",
    "            import time\n",
    "            time.sleep(attempts*19)\n",
    "\n",
    "    return result if result !='' else 'Failure'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8370a18-7440-4831-ad4c-dee6fd048712",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Test the function on a post that is a statement, without any embedded questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261a0e7-cd6b-4934-b125-ecc37eec610b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "segment_post_into_questions(\"'Catalysts play a critical role in lowering the activation energy of a chemical reaction, allowing it to proceed more quickly and efficiently without being consumed in the process. In industrial applications like the Haber process for ammonia production, iron-based catalysts are used to facilitate the reaction between nitrogen and hydrogen under optimized temperature and pressure conditions.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe41edb-3ed3-4d0b-9c58-e12de5726738",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Test the function on a \"normal\" type of post, with one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f75914-d09b-4172-84a0-77556512151a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "segment_post_into_questions(\"Hey team, let's discuss how water's unique properties (like high specific heat, surface tension, and being a universal solvent) arise from its molecular structure and hydrogen bonding. <at id=\\\"0\\\">Sam Carter</at>, <at id=\\\"1\\\">Kim Lee</at>, what are your thoughts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9037abc6-3d1c-4f99-b6cf-e1edb9e252e9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Apply the function to our dataset of ~1k conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218912ce-e828-43ef-b5ea-1dbf0e7ad853",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#df_segmented = df.head(30)\n",
    "df_segmented = df\n",
    "df_segmented['questions'] = df_segmented['parent_post_text'].apply(segment_post_into_questions)\n",
    "df_segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb855b-5993-4b4f-ada9-e2f2c55f2a3c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Sample result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441c5cf-12dd-4eb3-8776-b5ed74d4d284",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Display full content of parent_post_text in row with id = messages_Chemistry_20250320_055839_07f3eb8e\n",
    "questions = df_segmented.loc[df_segmented['Id'] == 'messages_Chemistry_20250320_053736_cf657902', 'questions'].values\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f695d2-5a09-423b-86e9-803e245a9fa1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Expand to have an individual row for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a884c-8cb8-4b46-85f8-bf96fe4bef6f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Function to expand questions into multiple rows\n",
    "def expand_questions(value):\n",
    "    try:\n",
    "        questions_json = json.loads(value)\n",
    "        return questions_json\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    " \n",
    "df_segmented['questions_json'] = df_segmented['questions'].apply(expand_questions)\n",
    "expanded_df = df_segmented.explode('questions_json').reset_index(drop=True)\n",
    "expanded_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f80f7-db29-4aae-acce-36a33cdad090",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "expanded_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686d40e-2bd0-44e1-8882-afbe40cb8148",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Clean rows that have no questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142dc58-7cd3-4f70-a87e-3c64b6f9f022",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the dataframe to non-empty values of questions_json\n",
    "filtered_df = expanded_df[expanded_df['questions_json'].notna()]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa967a19-dd87-4251-a57d-9973e40317a3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c05045-8aac-47cb-b058-149dfc99dc3a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Display full content of parent_post_text in row with id = messages_Chemistry_20250320_055839_07f3eb8e\n",
    "questions = filtered_df.loc[filtered_df['Id'] == 'messages_Chemistry_20250320_051407_e3396f52', 'questions_json'].values\n",
    "print(questions, type(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cfd017-a9f4-42bb-bfca-73750bbf1a3c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Crack JSON open to get the refined questions out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4386d-0290-44d8-bba6-118966749791",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def expand_questions(value): \n",
    "    try: \n",
    "        return value['question']\n",
    "    except json.JSONDecodeError:\n",
    "        return ''\n",
    "\n",
    "# Make a column 'question_refined' with value of 'question'\n",
    "filtered_df['question_refined'] = filtered_df['questions_json'].apply(expand_questions)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fc71a-cb49-402b-a00c-ca18fa48b07b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2a48e-4a20-46aa-a557-6ad4e4a8a0f1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Shuffle rows in the dataset - don't have to do it, but makes it more interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0b446-bef6-4424-aabd-c349d998c306",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_df = filtered_df.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab23f77-2b11-4206-a113-e9ef51f8de01",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Embed the content of questions with \"text-embedding-ada-002\" from Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79b842-2372-4bf8-8aea-8fe67e375ffb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def generate_embedding(text): \n",
    "    deployment_id = \"text-embedding-ada-002\"     \n",
    "    attempts = 0\n",
    "    result = []\n",
    "    \n",
    "    print(f\"start to work on input: {text}\")\n",
    "    while attempts < 10 and result == []:\n",
    "        try:\n",
    "            attempts+=1 \n",
    "            query_embedding = openai.Embedding.create(deployment_id=deployment_id, input=text) \n",
    "            result = query_embedding.data[0].embedding \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"sleeping, attempt {attempts}\")\n",
    "            import time\n",
    "            time.sleep(attempts*19)\n",
    "\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3f712-59af-4c1a-8602-3f2a765ebef1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Apply the embeddings function to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846c9aa-ffd9-4481-a1bb-4dee822618c4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_df['embeddings'] = shuffled_df['question_refined'].apply(generate_embedding)\n",
    "shuffled_df_with_embeddings = shuffled_df\n",
    "shuffled_df_with_embeddings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e59bb3-4d69-4d1a-bc52-025b633ecd67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Check that we have cleaned all rows which have no questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b9c45-8e51-4259-805b-73e90b9731ec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "pdf = shuffled_df_with_embeddings\n",
    "none_count = pdf['question_refined'].isnull().sum()\n",
    "none_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceef8cc-9ba4-4393-a55e-eb95e18c0698",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# fix this!\n",
    "pdf_cleaned = pdf.dropna(subset=['question_refined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede6db4-813f-4e70-a556-86d4c178bf3e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Transform to numeric format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cf589-4872-4422-99a8-51e54f3bb6c4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Convert each inner list to numeric values\n",
    "pdf_cleaned['Embedding_list'] = pdf_cleaned['embeddings'].apply(lambda x: np.array([pd.to_numeric(i, errors='coerce') for i in x])) \n",
    "\n",
    "pdf_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc3d99-0c67-41d6-b6e6-9dc7dea590df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# THIS IS IT!! The Beautiful Clustering.... ðŸŒ¸ðŸª·ðŸŒ¼ðŸŒ»ðŸŒ·ðŸª»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8478a8-0ce0-4915-b92a-46d8642bc3dc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the cosine distance matrix\n",
    "cosine_distances = pdist(pdf_cleaned['Embedding_list'].tolist(), metric='cosine')\n",
    "\n",
    "\n",
    "# Perform hierarchical clustering using the cosine distance matrix\n",
    "Z = linkage(cosine_distances, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Cosine Similarity)')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321163a-eb8b-4a3e-b7db-9d1ccacbfd9d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Let's pick 100 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1463c-0d73-4e57-8894-a7f6fded7bdc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "clusters_count = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a59a78-bccd-4c27-aeda-a4527c8d9ff1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Print which questions ended up in each cluster, from 1st to 100th..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1f433-5dbb-42b7-b005-1df1b0315b95",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Function to display clusters and their members\n",
    "def display_clusters(Z, pdf_cleaned, num_clusters=200):\n",
    "    # Create clusters from the linkage matrix\n",
    "    clusters = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Add cluster labels to the DataFrame\n",
    "    pdf_cleaned['Cluster'] = clusters\n",
    "    \n",
    "    # Display clusters and their members\n",
    "    for cluster in set(clusters):\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        cluster_data = pdf_cleaned[pdf_cleaned['Cluster'] == cluster]\n",
    "        for index, row in cluster_data.iterrows():\n",
    "            print(f\"\\n\\nThread id:  {row['Id']}, Question: {row['question_refined']}, Theme:  {row['Theme']}, Original post: {row['parent_post_text']}\")\n",
    "\n",
    "# Display clusters and their members\n",
    "display_clusters(Z, pdf_cleaned, num_clusters=clusters_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d1c7e-9792-42ea-a085-d326419f6b3a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now display the contents of each cluster in descending order by size (to help us find most frequently asked questions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc33114-bd36-4a25-a1c0-be32f28fc701",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of members in each cluster\n",
    "cluster_counts = pdf_cleaned['Cluster'].value_counts()\n",
    "\n",
    "# Print the number of elements in each cluster and examples of questions inside the clusters\n",
    "print(\"\\nNumber of elements in each cluster and examples of questions inside the clusters:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"\\n\\nCluster {cluster}: {count} elements\")\n",
    "    cluster_data = pdf_cleaned[pdf_cleaned['Cluster'] == cluster]\n",
    "    for index, row in cluster_data.iterrows():  # Displaying first 3 examples from each cluster\n",
    "        print(f\"\\n\\nThread id:  {row['Id']}, Question: {row['question_refined']}, Theme:  {row['Theme']}, Original post: {row['parent_post_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60776b12-51e2-4089-b5ad-5d68e9553340",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Summarize the contents of each cluster - compress groups of questions with the help of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9144c-65d9-4c25-b3a9-8b2a4afbc367",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def find_common_theme(text, verbose=1, questions_only = 1):\n",
    "    # Craft the prompt\n",
    "    if questions_only:\n",
    "        prompt = f\"Please find common thene in the given questions and context:\\\n",
    "                \\n\\n{text}\\n\\n\\\n",
    "                Based on that, formulate one aggregated most popular question (ideally) or few questions based on trends in the provided data. Answer Question(s): <> \"\n",
    "    else: \n",
    "        prompt = f\"Please find common thene in the given questions and context:\\\n",
    "                \\n\\n{text}\\n\\n\\\n",
    "                Formulate common theme as a topic,  a set of keywords and  one aggregated most popular question (ideally) or few questions based on trends in the provided data. Answer as Topic: <>,  Keywords: <>, Question(s): <> \"\n",
    "        \n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a helpful  assistant who will be provided text information to generate FAQ \\\n",
    "                \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"question:\" + prompt,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    attempts = 0\n",
    "    result = ''\n",
    "    \n",
    "    #print(f\"start to work on input: {text}\")\n",
    "    while attempts < 10 and result == '':\n",
    "        try:\n",
    "            attempts+=1 \n",
    "\n",
    "            response = openai.ChatCompletion.create(\n",
    "                #deployment_id='gpt-35-turbo-0125', # see the note in the cell below for an alternative deployment_id.\n",
    "                deployment_id=\"gpt-4-32k\",\n",
    "                messages= messages,\n",
    "                temperature=0,\n",
    "            ) \n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            #print(result)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"sleeping, attempt {attempts}\")\n",
    "            import time\n",
    "            time.sleep(attempts*19)\n",
    "\n",
    "    return result if result !='' else 'Failure' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13d604-b5e8-4848-a92c-0c6671460f6d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Apply the function only to clusters that have 5 or more questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22219eac-1f7c-4bec-97ad-f4566908e984",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of members in each cluster\n",
    "cluster_counts = pdf_cleaned['Cluster'].value_counts()\n",
    "\n",
    "# Filter clusters with more than 5 entries and sort in descending order\n",
    "filtered_clusters = cluster_counts[cluster_counts > 5].sort_values(ascending=False)\n",
    "\n",
    "themes = []\n",
    "# Print the number of elements in each cluster and concatenated questions\n",
    "#print(\"\\nNumber of elements in each cluster and examples of questions inside the clusters (more than 5 entries):\")\n",
    "for cluster, count in filtered_clusters.items():\n",
    "    #print(f\"\\n\\nCluster {cluster}: {count} elements\")\n",
    "    cluster_data = pdf_cleaned[pdf_cleaned['Cluster'] == cluster]\n",
    "    questions = []\n",
    "    parent_posts = []\n",
    "    for index, row in cluster_data.iterrows():\n",
    "        questions.append(f\"Question: {row['question_refined']}\")\n",
    "        # print(f\"\\n\\nThread id:  {row['Id']}, Question: {row['question_refined']}, Theme:  {row['Theme']}, Original post: {row['parent_post_text']}\")\n",
    "        parent_posts.append(f\"Post: {row['parent_post_text']}\")\n",
    "      \n",
    "    result = \"  Questions: \" + \" | \".join(questions)\n",
    "    theme = find_common_theme(result)\n",
    "    parent_posts = \"  Parent posts: \" + \" | \".join(parent_posts)    \n",
    "    themes.append((theme, count))\n",
    "    #print(f\"\\n{theme}; \\n\\n {parent_posts}\")\n",
    "\n",
    "print(\"~~~FAQ~~~~\")\n",
    "for theme, num in themes:\n",
    "    print(theme, f\" (asked {num} times)\\n =================== \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c00fc-0c98-4e1b-abe5-e2a4b02f5c36",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Augment with answers - homework exercise ðŸ˜ŠðŸ™Œ"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "environment": {
    "environmentId": "14382343-a180-4358-92bf-496f121089a3",
    "workspaceId": "072ab5bf-d090-4ad2-84a0-58ec1073e98e"
   },
   "lakehouse": {
    "default_lakehouse": "43d8dbf0-7a68-45a7-b625-9f99518e2627",
    "default_lakehouse_name": "knowledge",
    "default_lakehouse_workspace_id": "bf0d7a8e-68ed-4175-815a-44903de71fb4"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
