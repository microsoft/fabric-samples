{
    "cells": [{
            "cell_type": "markdown",
            "source": ["# Module 1: Ingest data into lakehouse using Spark\r\n", "**Lakehouse**:\r\n", "A lakehouse is a collection of files/folders/tables that represent a database over a data lake used by \r\n", "the Spark engine and SQL engine for big data processing and that includes enhanced capabilities for \r\n", "ACID transactions when using the open-source Delta formatted tables.\r\n", "\r\n", "**Delta Lake**: Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata management, and batch and streaming data processing to Apache Spark. A Delta Lake table is a data table format that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata management."],
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Connect to Azure Open Datasets Container and read NYC Taxi yellow cab dataset\r\n", "[Azure Open Datasets](https://learn.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets) are curated public datasets that you can use to add scenario-specific features to machine learning solutions for more accurate models. Open Datasets are in the cloud on Microsoft Azure Storage and can be accessed by a variety of methods including, Apache Spark, REST API, Datafactory and other tools."],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["# Azure storage access info for open datasets yellow cab\r\n", "storage_account = \"azureopendatastorage\"\r\n", "container = \"nyctlc\"\r\n", "\r\n", "sas_token = r\"\" # Blank since container is Anonymous access\r\n", "\r\n", "# Set Spark config to access  blob storage\r\n", "spark.conf.set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (container, storage_account),sas_token)\r\n", "\r\n", "dir = \"yellow\"\r\n", "year = 2016\r\n", "months = \"1,2,3,4\"\r\n", "wasbs_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/{dir}\"\r\n", "df = spark.read.parquet(wasbs_path)\r\n", "\r\n", "# Filter data by year and months\r\n", "filtered_df = df.filter(f\"puYear = {year} AND puMonth IN ({months})\")"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "34f8c4ff-dac1-4f27-a99d-4cffd3d05fec",
                            "statement_id": 3,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T05:44:51.6084919Z",
                            "session_start_time": "2023-05-08T05:44:52.266799Z",
                            "execution_start_time": "2023-05-08T05:45:01.9006684Z",
                            "execution_finish_time": "2023-05-08T05:45:07.4994725Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 2,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 0,
                                        "dataRead": 0,
                                        "rowCount": 0,
                                        "jobId": 8,
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 3:\n# Azure storage access info for open datasets yellow cab\nstorage_account = \"azureopendatastorage\"\ncontainer = \"nyctlc\"\n\nsas_token = r\"\" # Blank since container is Anonymous access\n\n# Set Spark config to access  blob storage\nspark.conf.set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (container, storage_account),sas_token)\n\ndir = \"yellow\"\nyear = 2016\nmonths = \"1,2,3,4\"\nwasbs_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/{dir}\"\ndf = spark.read.parquet(wasbs_path)\n\n# Filter data by year and months\nfiltered_df = df.filter(f\"puYear = {year} AND puMonth IN ({months})\")",
                                        "submissionTime": "2023-05-08T05:45:06.316GMT",
                                        "completionTime": "2023-05-08T05:45:06.604GMT",
                                        "stageIds": [11],
                                        "jobGroup": "3",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 0,
                                        "rowCount": 0,
                                        "jobId": 7,
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "description": "Listing leaf files and directories for 35 paths:<br/>wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2041, ...",
                                        "submissionTime": "2023-05-08T05:45:02.245GMT",
                                        "completionTime": "2023-05-08T05:45:06.133GMT",
                                        "stageIds": [10],
                                        "jobGroup": "3",
                                        "status": "SUCCEEDED",
                                        "numTasks": 35,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 35,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 35,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "814e0a95-8095-4460-9871-d43b10488f94"
                        },
                        "text/plain": "StatementMeta(, 34f8c4ff-dac1-4f27-a99d-4cffd3d05fec, 3, Finished, Available)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 1,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["### Write Spark dataframe to lakehouse delta table"],
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["**Enable Vorder and Optimized Delta Write**\r\n", "\r\n", "- **VOrder**: Trident includes Microsoft's VOrder engine. VOrder writer optimizes the Delta Lake parquet files resulting in 3x-4x compression improvement and up to 10x performance acceleration over Delta Lake files not optimized using VOrder while still maintaining full Delta Lake and PARQUET format compliance.<p>\r\n", "- **Optimize write**: Spark in Trident includes an Optimize Write feature that reduces the number of files written and targets to increase individual file size of the written data. It dynamically optimizes files during write operations generating files with a default 128 MB size. The target file size may be changed per workload requirements using configurations.\r\n", "\r\n", "These configs can be applied at a session level(as spark.conf.set in a notebook cell) as demonstrated in the following code cell, or at workspace level which is applied automatically to all spark sessions created in the workspace. The workspace level Apache Spark configuration can be set at:\r\n", "- _Workspace settings >> Data Engineering/Sceience >> Spark Compute >> Spark Properties >> Add_\r\n"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\n", "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "34f8c4ff-dac1-4f27-a99d-4cffd3d05fec",
                            "statement_id": 4,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T05:44:51.6120209Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T05:45:08.4866305Z",
                            "execution_finish_time": "2023-05-08T05:45:08.8203513Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 0,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "f6ad07b2-152d-45eb-b5d9-79cc495bd9ba"
                        },
                        "text/plain": "StatementMeta(, 34f8c4ff-dac1-4f27-a99d-4cffd3d05fec, 4, Finished, Available)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 2,
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["table_name = \"nyctaxi_raw\"\r\n", "filtered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\r\n", "print(f\"Spark dataframe saved to delta table: {table_name}\")"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "34f8c4ff-dac1-4f27-a99d-4cffd3d05fec",
                            "statement_id": 5,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T05:44:51.6132844Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T05:45:09.7777038Z",
                            "execution_finish_time": "2023-05-08T05:46:12.8474415Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 6,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 0,
                                        "dataRead": 4795,
                                        "rowCount": 50,
                                        "jobId": 14,
                                        "name": "toString at String.java:2994",
                                        "description": "Delta: Job group for statement 5:\ntable_name = \"nyctaxi_raw\"\nfiltered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\nprint(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 0",
                                        "submissionTime": "2023-05-08T05:46:11.551GMT",
                                        "completionTime": "2023-05-08T05:46:11.589GMT",
                                        "stageIds": [19, 20, 18],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 52,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 51,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 2,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 4795,
                                        "dataRead": 16511,
                                        "rowCount": 67,
                                        "jobId": 13,
                                        "name": "toString at String.java:2994",
                                        "description": "Delta: Job group for statement 5:\ntable_name = \"nyctaxi_raw\"\nfiltered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\nprint(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 0",
                                        "submissionTime": "2023-05-08T05:46:10.897GMT",
                                        "completionTime": "2023-05-08T05:46:11.531GMT",
                                        "stageIds": [16, 17],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 51,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 50,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 16511,
                                        "dataRead": 27370,
                                        "rowCount": 34,
                                        "jobId": 12,
                                        "name": "toString at String.java:2994",
                                        "description": "Delta: Job group for statement 5:\ntable_name = \"nyctaxi_raw\"\nfiltered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\nprint(f\"Spark dataframe saved to delta table: {table_name}\"): Compute snapshot for version: 0",
                                        "submissionTime": "2023-05-08T05:46:10.511GMT",
                                        "completionTime": "2023-05-08T05:46:10.646GMT",
                                        "stageIds": [15],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 0,
                                        "rowCount": 0,
                                        "jobId": 11,
                                        "name": "",
                                        "description": "Job group for statement 5:\ntable_name = \"nyctaxi_raw\"\nfiltered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\nprint(f\"Spark dataframe saved to delta table: {table_name}\")",
                                        "submissionTime": "2023-05-08T05:46:09.392GMT",
                                        "completionTime": "2023-05-08T05:46:09.392GMT",
                                        "stageIds": [],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 0,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 0,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 0,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 1068254400,
                                        "dataRead": 2979629839,
                                        "rowCount": 92859236,
                                        "jobId": 10,
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 5:\ntable_name = \"nyctaxi_raw\"\nfiltered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\nprint(f\"Spark dataframe saved to delta table: {table_name}\")",
                                        "submissionTime": "2023-05-08T05:45:34.490GMT",
                                        "completionTime": "2023-05-08T05:46:09.135GMT",
                                        "stageIds": [13, 14],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 28,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 14,
                                        "numSkippedTasks": 14,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 14,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 2979629839,
                                        "dataRead": 1422565837,
                                        "rowCount": 92859236,
                                        "jobId": 9,
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 5:\ntable_name = \"nyctaxi_raw\"\nfiltered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\nprint(f\"Spark dataframe saved to delta table: {table_name}\")",
                                        "submissionTime": "2023-05-08T05:45:12.448GMT",
                                        "completionTime": "2023-05-08T05:45:34.445GMT",
                                        "stageIds": [12],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 14,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 14,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 14,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "d4a774b0-e746-4170-9d2a-e4501b7db2aa"
                        },
                        "text/plain": "StatementMeta(, 34f8c4ff-dac1-4f27-a99d-4cffd3d05fec, 5, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["Spark dataframe saved to delta table: nyctaxi_raw\n"]
                }
            ],
            "execution_count": 3,
            "metadata": {}
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "language": "Python",
            "display_name": "Synapse PySpark"
        },
        "widgets": {},
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "enableDebugMode": false,
                "conf": {}
            }
        },
        "notebook_environment": {},
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
