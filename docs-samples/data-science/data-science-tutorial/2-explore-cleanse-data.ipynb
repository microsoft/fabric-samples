{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5189eacc",
   "metadata": {},
   "source": [
    "# Part 2: Explore data\n",
    "\n",
    "In this tutorial, you'll use `seaborn`, which is a Python data visualization library that provides a high-level interface for building visuals on dataframes and arrays. For more information about `seaborn`, see [seaborn: statistical data visualization](https://seaborn.pydata.org/).  \n",
    "\n",
    "You'll also use [Data Wrangler]((https://aka.ms/fabric/datawrangler)), a notebook-based tool that provide you with an immersive experience to conduct exploratory data analysis and cleansing.\n",
    "\n",
    "In this tutorial you learn to perform the following actions:\n",
    "\n",
    "1. Read the data stored from a delta table in the lakehouse.\n",
    "1. Convert a Spark DataFrame to Pandas DataFrame, which python visualization libraries support.\n",
    "1. Use Data Wrangler to perform initial data cleaning and transformation.\n",
    "1. Perform exploratory data analysis using `seaborn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a4003",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Complete [Part 1: Ingest data](https://learn.microsoft.com/fabric/data-science/tutorial-data-science-ingest-data).\n",
    "- Attach the same lakehouse you used in Part 1 to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308ceaf",
   "metadata": {},
   "source": [
    "## Read raw data from the lakehouse\n",
    "\n",
    "Read raw data from the **Files** section of the lakehouse. You uploaded this data in the previous notebook. Make sure you have attached the same lakehouse you used in Part 1 to this notebook before you run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd34fd-047c-44d0-884a-4941482bd5fb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "ms_comment_ranges": {},
    "ms_comments": [],
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"Files/churn/raw/churn.csv\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a5c225-f482-4703-9b91-5cb6ba1970ae",
   "metadata": {},
   "source": [
    "## Create a pandas DataFrame from the dataset\n",
    "\n",
    "Convert the spark DataFrame to pandas DataFrame for easier processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550d94b-552e-41fd-b23f-3caa064f842a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "ms_comment_ranges": {},
    "ms_comments": [],
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc3a2a-8497-4272-8e5f-bc0e22b8d588",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Display raw data\n",
    "\n",
    "Explore the raw data with `display`, do some basic statistics and show chart views. You first need to import required libraries for data visualization such as `seaborn`, which is a Python data visualization library to provide a high-level interface for building visuals on DataFrames and arrays. Learn more about [`seaborn`](https://seaborn.pydata.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73290de5-ed00-415a-b546-5f4a7586cff7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)})\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import rc, rcParams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb983a-dc4e-495f-9143-321099d2a7c6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd45b80-3619-493e-8aaf-d338e2b5149b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Use Data Wrangler to perform initial data cleansing\n",
    "\n",
    "Data Wrangler is a notebook-based tool that provides an immersive experience to conduct exploratory data analysis and cleansing. \n",
    "\n",
    "To use Data Wrangler for performing initial data cleansing, follows the steps in [the online documentation](https://learn.microsoft.com/fabric/data-science/tutorial-data-science-explore-notebook#data-wrangler). \n",
    "\n",
    "Once you complete these steps, you'll see a new notebook code cell here, which was generated by Data Wrangler. Run that code cell to perform the operations you explored in Data Wrangler.\n",
    "\n",
    "If you didn't use Data Wrangler, you can instead use this next code cell. \n",
    "\n",
    "This code is similar to the code produced by Data Wrangler, but adds in the argument `inplace=True` to each of the generated steps. By setting `inplace=True`, pandas will overwrite the original DataFrame instead of producing a new DataFrame as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f2fa9-7d75-4c05-87d4-cf9c0d7077cc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Modified version of code generated by Data Wrangler \n",
    "# Modification is to add in-place=True to each step\n",
    "\n",
    "# Define a new function that include all above Data Wrangler operations\n",
    "def clean_data(df):\n",
    "    # Drop rows with missing data across all columns\n",
    "    df.dropna(inplace=True)\n",
    "    # Drop duplicate rows in columns: 'RowNumber', 'CustomerId'\n",
    "    df.drop_duplicates(subset=['RowNumber', 'CustomerId'], inplace=True)\n",
    "    # Drop columns: 'RowNumber', 'CustomerId', 'Surname'\n",
    "    df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922660b-dd45-4c2d-8a75-7431fefffb0c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Explore the data\n",
    "\n",
    "Display some summaries and visualizations of the cleaned data.\n",
    "\n",
    "### Determine categorical, numerical, and target attributes\n",
    "\n",
    "Use this code to determine categorical, numerical, and target attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb4922-607e-4f4e-bdb2-bda535acb94c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Determine the dependent (target) attribute\n",
    "dependent_variable_name = \"Exited\"\n",
    "print(dependent_variable_name)\n",
    "# Determine the categorical attributes\n",
    "categorical_variables = [col for col in df_clean.columns if col in \"O\"\n",
    "                        or df_clean[col].nunique() <=5\n",
    "                        and col not in \"Exited\"]\n",
    "print(categorical_variables)\n",
    "# Determine the numerical attributes\n",
    "numeric_variables = [col for col in df_clean.columns if df_clean[col].dtype != \"object\"\n",
    "                        and df_clean[col].nunique() >5]\n",
    "print(numeric_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640a89c-6a64-4978-89f1-5a45cdd1303e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### The five-number summary \n",
    "\n",
    "Show the five-number summary (the minimum score, first quartile, median, third quartile, the maximum score) for the numerical attributes, using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45982d4-de27-43b5-b826-e93730d7e66c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_num_cols = df_clean[numeric_variables]\n",
    "sns.set(font_scale = 0.7) \n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 3, gridspec_kw =  dict(hspace=0.3), figsize = (17,8))\n",
    "fig.tight_layout()\n",
    "for ax,col in zip(axes.flatten(), df_num_cols.columns):\n",
    "    sns.boxplot(x = df_num_cols[col], color='green', ax = ax)\n",
    "fig.delaxes(axes[1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320fd15-64eb-493e-9b60-500ad4535435",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Distribution of exited and nonexited customers \n",
    "\n",
    "Show the distribution of exited versus nonexited customers across the categorical attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4811cab-6cdc-4014-a0ab-8f90232e6603",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "attr_list = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Tenure']\n",
    "fig, axarr = plt.subplots(2, 3, figsize=(15, 4))\n",
    "for ind, item in enumerate (attr_list):\n",
    "    sns.countplot(x = item, hue = 'Exited', data = df_clean, ax = axarr[ind%2][ind//2])\n",
    "fig.subplots_adjust(hspace=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075fddc-1cce-4c64-8d17-499a309440d7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Distribution of numerical attributes\n",
    "\n",
    "Show the frequency distribution of numerical attributes using histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8626b081-a301-4cfa-9db3-38ab8524879d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "columns = df_num_cols.columns[: len(df_num_cols.columns)]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18, 8)\n",
    "length = len(columns)\n",
    "for i,j in itertools.zip_longest(columns, range(length)):\n",
    "    plt.subplot((length // 2), 3, j+1)\n",
    "    plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n",
    "    df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n",
    "    plt.title(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e1166-0e88-4baa-ac2c-b56b7eeeb7c4",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Perform feature engineering\n",
    "\n",
    "Perform feature engineering to generate new attributes based on current attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3e0c5-db7b-44bf-b11b-8d323934e151",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_clean[\"NewTenure\"] = df_clean[\"Tenure\"]/df_clean[\"Age\"]\n",
    "df_clean[\"NewCreditsScore\"] = pd.qcut(df_clean['CreditScore'], 6, labels = [1, 2, 3, 4, 5, 6])\n",
    "df_clean[\"NewAgeScore\"] = pd.qcut(df_clean['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "df_clean[\"NewBalanceScore\"] = pd.qcut(df_clean['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\n",
    "df_clean[\"NewEstSalaryScore\"] = pd.qcut(df_clean['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be9b0c-3f67-461f-8bd7-cd0450b2bc1e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Data Wrangler can also be used to perform one-hot encoding.  To do so, re-open Data Wrangler.  This time, select the `df_clean` data. Then select **One-hot encode** under **Formulas**.  \n",
    "\n",
    "Or use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b924a18-faf5-4b0b-8b57-3480f2f93083",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This is the same code that Data Wrangler will generate\n",
    " \n",
    "import pandas as pd\n",
    " \n",
    "def clean_data(df_clean):\n",
    "    # One-hot encode columns: 'Geography', 'Gender'\n",
    "    df_clean = pd.get_dummies(df_clean, columns=['Geography', 'Gender'])\n",
    "    return df_clean\n",
    " \n",
    "df_clean_1 = clean_data(df_clean.copy())\n",
    "df_clean_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebcb1b-29cd-4c2b-90ce-e6556388456c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Summary of observations from the exploratory data analysis\n",
    "\n",
    "- Most of the customers are from France comparing to Spain and Germany, while Spain has the lowest churn rate comparing to France and Germany.\n",
    "- Most of the customers have credit cards.\n",
    "- There are customers whose age and credit score are above 60 and below 400, respectively, but they can't be considered as outliers.\n",
    "- Very few customers have more than two of the bank's products.\n",
    "- Customers who aren't active have a higher churn rate.\n",
    "- Gender and tenure years don't seem to have an impact on customer's decision to close the bank account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f77d9-1167-460e-a3ba-3268acafd285",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create a delta table for the cleaned data\n",
    "\n",
    "You'll use this data in the next notebook of this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061366a-dd4f-43e1-9b81-081f43d20378",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"df_clean\"\n",
    "# Create Spark DataFrame from pandas\n",
    "sparkDF=spark.createDataFrame(df_clean_1) \n",
    "sparkDF.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n",
    "print(f\"Spark dataframe saved to delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae96c4",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "Proceed to [Part 3: Train and register machine learning models](https://learn.microsoft.com/fabric/data-science/tutorial-data-science-train-models) to use the `df_clean` data to train and register machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
