{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cce6d9-bb3c-41be-8e30-06f2bf28020f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Create, evaluate, and score a recommendation system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4082704-8c99-4a23-8f26-a0cde6321bed",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Introduction\n",
    "In this notebook, you walk through the Microsoft Fabric data engineering and data science workflow with an end-to-end tutorial. The scenario is to build a book recommendation system for the users based on their preferences.\n",
    "\n",
    "A recommendation system is a class of machine learning algorithms that offers relevant suggestions to the user. Considering a user's past preferences or ratings, a recommendation system would suggest new items to the user that could include movies to watch, books to read, products to buy, etc. Several types of recommendation algorithms exist, but in this notebook you use a model-based collaborative filtering algorithm called **Alternating Least Squares (ALS) matrix factorization**.\n",
    "\n",
    "<img src=\"https://negustpublicblob.blob.core.windows.net/public/recommenders.png\" style=\"width:600px;\"/>\n",
    "\n",
    "ALS attempts to estimate a ratings matrix $R$ as a product of two lower-rank matrices known as \"factor\" matrices such that $R = U^T V$  where $U$ and $V$ can be called user and item matrix, respectively. The general approach is iterative and during each iteration, one of the factor matrices is held constant, while the other is solved using the least squares. Applying this approach alternately to the matrices $U$ and $V$ is guaranteed to monotonically decrease the overall cost function and hence you can iteratively improve the matrix factorization.\n",
    "\n",
    "<img src=\"https://negustpublicblob.blob.core.windows.net/public/Matrixfactor.svg\" style=\"width:600px;\"/>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Please note that this notebook can be modified to use custom data by adjusting certain parameters that will be discussed below. \n",
    "\n",
    "The summary of the main steps you take in this notebook are as following:\n",
    "\n",
    "1. Load and process the data\n",
    "2. Understand the data using exploratory data analysis\n",
    "3. Train a machine learning model using an Alternating Least Squares (ALS) model and track experiments using MLflow and Fabric Autologging feature\n",
    "4. Save the final machine learning model and load it for scoring and making predictions\n",
    "\n",
    "## Prerequisites\n",
    "- Have a lakehouse added to this notebook. You download data from a public blob, and storing that in the lakehouse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8383948-b48e-4a42-a264-eaf5e52b43b1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The book recommendation dataset in this scenario consists of three separate datasets:\n",
    "\n",
    "**Books.csv**\n",
    "\n",
    "Each book is identified by its International Standard Book Number (ISBN), with invalid values already being removed. Additional information (e.g., title, author, publication year, publisher) has also been obtained from Amazon Web Services. If a book has multiple authors, only the first is listed. URLs are pointing to Amazon provided for cover images in three separate formats.\n",
    "\n",
    "|ISBN|Book-Title|Book-Author|Year-Of-Publication|Publisher|Image-URL-S|Image-URL-M|Image-URL-L|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|0195153448|Classical Mythology|Mark P. O. Morford|2002|Oxford University Press|http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg|http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg|http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg|\n",
    "|0002005018|Clara Callan|Richard Bruce Wright|2001|HarperFlamingo Canada|http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg|http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg|http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg|\n",
    "\n",
    "**Ratings.csv**\n",
    "\n",
    "Ratings for each book are either explicit (provided by users, and measured on a scale from 1-10), or implicit (observed without users providing input, and indicated by 0).\n",
    "\n",
    "|User-ID|ISBN|Book-Rating|\n",
    "|---|---|---|\n",
    "|276725|034545104X|0|\n",
    "|276726|0155061224|5|\n",
    "\n",
    "**Users.csv**\n",
    "\n",
    "User IDs, which have been anonymized and mapped to integers. Demographic data (i.e. Location, Age) are provided if available or else contain *null* values.\n",
    "\n",
    "|User-ID|Location|Age|\n",
    "|---|---|---|\n",
    "|1|\"nyc| new york| usa\"||\n",
    "|2|\"stockton| california| usa\"|18.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadadce4-349e-45bf-849f-7c3f61a7f734",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "**By defining below parameters, you can apply this notebook on different datasets easily.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b55b11-ccb1-4fbc-b689-5145002cc03c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "IS_CUSTOM_DATA = False  # if True, dataset has to be uploaded manually\n",
    "\n",
    "USER_ID_COL = \"User-ID\"  # must not be '_user_id' for this notebook to run successfully\n",
    "ITEM_ID_COL = \"ISBN\"  # must not be '_item_id' for this notebook to run successfully\n",
    "ITEM_INFO_COL = (\n",
    "    \"Book-Title\"  # must not be '_item_info' for this notebook to run successfully\n",
    ")\n",
    "RATING_COL = (\n",
    "    \"Book-Rating\"  # must not be '_rating' for this notebook to run successfully\n",
    ")\n",
    "IS_SAMPLE = True  # if True, use only <SAMPLE_ROWS> rows of data for training; otherwise use all data\n",
    "SAMPLE_ROWS = 5000  # if IS_SAMPLE is True, use only this number of rows for training\n",
    "\n",
    "DATA_FOLDER = \"Files/book-recommendation/\"  # folder containing the datasets\n",
    "ITEMS_FILE = \"Books.csv\"  # file containing the items information\n",
    "USERS_FILE = \"Users.csv\"  # file containing the users information\n",
    "RATINGS_FILE = \"Ratings.csv\"  # file containing the ratings information\n",
    "\n",
    "EXPERIMENT_NAME = \"aisample-recommendation\"  # mlflow experiment name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6accde-feb6-424c-ad6d-184945dd819b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Download data and store it in a Lakehouse\n",
    "\n",
    "The following code will download a publicly available version of the the dataset and then store it in a Fabric Lakehouse.\n",
    "\n",
    "**Please add a lakehouse to the notebook before running it. Failure to do so will result in an error.**\n",
    "\n",
    "For more information, see [Add a Lakehouse to the notebook](https://aka.ms/fabric/addlakehouse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c0b6b-359e-4103-bcdc-cc568cd62bfa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if not IS_CUSTOM_DATA:\n",
    "    # Download data files into lakehouse if it does not exist\n",
    "    import os, requests\n",
    "\n",
    "    remote_url = \"https://synapseaisolutionsa.blob.core.windows.net/public/Book-Recommendation-Dataset\"\n",
    "    file_list = [\"Books.csv\", \"Ratings.csv\", \"Users.csv\"]\n",
    "    download_path = f\"/lakehouse/default/{DATA_FOLDER}/raw\"\n",
    "\n",
    "    if not os.path.exists(\"/lakehouse/default\"):\n",
    "        raise FileNotFoundError(\n",
    "            \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n",
    "        )\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    for fname in file_list:\n",
    "        if not os.path.exists(f\"{download_path}/{fname}\"):\n",
    "            r = requests.get(f\"{remote_url}/{fname}\", timeout=30)\n",
    "            with open(f\"{download_path}/{fname}\", \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "    print(\"Downloaded demo data files into lakehouse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd39a7d-2eeb-41de-bde3-d5b6e866679a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You can use the following code to start recording the time it takes to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba87a73-5ca4-4f57-9701-ab23f2fbd6e4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Record the notebook running time\n",
    "import time\n",
    "\n",
    "ts = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60cbe7-fb02-4f88-b9f9-c426f9bc3f7d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Setup the MLflow experiment tracking\n",
    "\n",
    "Extending the MLflow autologging capabilities, autologging works by automatically capturing the values of input parameters and output metrics of a machine learning model as it is being trained. This information is then logged to your workspace, where it can be accessed and visualized using the MLflow APIs or the corresponding experiment in your workspace. For more information, see [Autologging](https://aka.ms/fabric-autologging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023beb35-488f-4555-aca5-7513f320e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup mlflow for experiment tracking\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.autolog(disable=True)  # disable mlflow autologging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b3bc9-06b3-4284-ae1f-8224a6e8af15",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Note that if you want to disable Microsoft Fabric autologging in a notebook session, call `mlflow.autolog()` and set `disable=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919804a2-0f91-42fe-a7f9-c37c6d6899c7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Read raw data from the lakehouse\n",
    "\n",
    "Once the right data has landed in the Lakehouse, you can read the three separate datasets into separate Spark DataFrames in the notebook. Note that the file paths in the cell below use the parameters that you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe5521-3fe1-4d15-91a7-10b18a2d7ebc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_items = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DATA_FOLDER}/raw/{ITEMS_FILE}\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "df_ratings = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DATA_FOLDER}/raw/{RATINGS_FILE}\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "df_users = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DATA_FOLDER}/raw/{USERS_FILE}\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c14a18-8e5a-4a04-8412-3a564c875bc5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Perform exploratory Data Analysis\n",
    "\n",
    "You can explore each of the DataFrames using the `display` command. This allows to view high-level statistics of the DataFrames and understand how different columns in the datasets are related to each other. To explore the datasets, you need to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d7422-e9f9-42f7-9d35-2d02a18c457e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()  # adjusting plotting style\n",
    "import pandas as pd  # dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e350f6-98a4-46f3-a331-cfb063de91d7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Book Dataset\n",
    "Let's look into the the DataFrame that stores the book data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d8d3c-c30f-4c6c-84e2-63476549ef3a",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"e734b53a-a334-4024-ad51-37e59483f53f\",\"activityId\":\"580049c8-7fe7-4b1f-b04e-cd2bf131d72f\",\"applicationId\":\"application_1689570789575_0001\",\"jobGroupId\":\"9\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_items, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1eaaa-f048-434a-90e9-048481fbf412",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You add the column named `_item_id` for later use which must only contain integers to suit our recommendation model. Therefore, use a `StringIndexer` to transform the `ITEM_ID_COL` into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c5b8d-ebb3-46ef-9276-f9f65d573456",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_items = (\n",
    "    StringIndexer(inputCol=ITEM_ID_COL, outputCol=\"_item_id\")\n",
    "    .setHandleInvalid(\"skip\")\n",
    "    .fit(df_items)\n",
    "    .transform(df_items)\n",
    "    .withColumn(\"_item_id\", F.col(\"_item_id\").cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e51b8-be76-437d-8964-5f7834553721",
   "metadata": {},
   "source": [
    "Let's take a look at these values to ensure that the content of the `_item_id` column increases successively as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234a2eb-8f97-4419-bf5a-c7ecd9bdd63f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_items.sort(F.col(\"_item_id\").desc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655a377-f9da-4c20-b042-3b45f54e4e6a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Plot the top 10 authors with the maximum number of books.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01e6ad-d2aa-4b0a-a9d5-86772453a65b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_books = df_items.toPandas() # Create a pandas dataframe from the spark dataframe for visualization\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(y=\"Book-Author\",palette = 'Paired', data=df_books,order=df_books['Book-Author'].value_counts().index[0:10])\n",
    "plt.title(\"Top 10 authors with maximum number of books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a1f6e-b930-4adf-a952-d821a9aeb5d9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Agatha Christie is the leading author with over 600 books, followed by William Shakespeare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71577c6c-1e24-4556-bc5e-20757849b529",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### User Dataset\n",
    "Let's take a look at the DataFrame that stores the user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493804b-6faa-46ab-8f00-f27ef985a09e",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"e734b53a-a334-4024-ad51-37e59483f53f\",\"activityId\":\"580049c8-7fe7-4b1f-b04e-cd2bf131d72f\",\"applicationId\":\"application_1689570789575_0001\",\"jobGroupId\":\"13\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_users, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fc998-b2f0-4761-819f-717ad967d7f5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Remove the rows with missing `User-ID` values to clean up the data prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfb117-d761-467a-95d1-445937f3e598",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_users = df_users.dropna(subset=(USER_ID_COL))\n",
    "display(df_users, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc46c0-37a4-457b-8077-70a59119d9d6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Similarly, add the column named `_user_id` for later use which must only contain integers to suit our recommendation model. Therefore, use a `StringIndexer` to transform the `USER_ID_COL` into indices.\n",
    "\n",
    "**Note**: In this scenario, the book dataset already consists of an integer column named `User-ID`. However, in order to make this notebook more robust, add a `_user_id` column for compatibility with different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8d227-31a2-4dcb-9e93-91f926e990c6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_users = (\n",
    "    StringIndexer(inputCol=USER_ID_COL, outputCol=\"_user_id\")\n",
    "    .setHandleInvalid(\"skip\")\n",
    "    .fit(df_users)\n",
    "    .transform(df_users)\n",
    "    .withColumn(\"_user_id\", F.col(\"_user_id\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "display(df_users.sort(F.col(\"_user_id\").desc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b848d8-6056-43bd-8e29-a2e5381224bc",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Rating Dataset\n",
    "Let's take a look at the DataFrame that stores the rating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec6bae-93db-4db0-892a-9be3bc1ccf7d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_ratings, summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07c665-94df-42bd-a285-ffa2b706ce4d",
   "metadata": {},
   "source": [
    "Let's collect the distinct ratings and save them to a list named `ratings` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22adf2a-2b2a-4625-a4ae-aae616b166d5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ratings = [i[0] for i in df_ratings.select(RATING_COL).distinct().collect()]\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220f032-b83a-45af-bf85-9015efe1715e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Plot the top 10 books with the highest ratings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872775eb-9a61-49d9-a7ee-656bbfdc1ff1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(y=\"Book-Title\",palette = 'Paired',data= df_books, order=df_books['Book-Title'].value_counts().index[0:10])\n",
    "plt.title(\"Top 10 books per number of ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705aa6f6-26aa-4e18-9a15-e49025dadfec",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\"Selected Poems\" is most favorable among users according to ratings. The books \"Adventures of Huckleberry Finn\", \"The Secret Garden\", and \"Dracula\", have the same rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac762b1f-e728-4d50-8633-95e2d1a41960",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Merge all Datasets\n",
    "\n",
    "Once looked into all three DataFrames corresponding to Book, User and Rating datasets, merge all three DataFrames into one DataFrame for a more comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14f28b-d564-40b5-b6f9-4646e90d01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_ratings.join(df_users, USER_ID_COL, \"inner\").join(\n",
    "    df_items, ITEM_ID_COL, \"inner\"\n",
    ")\n",
    "df_all_columns = [\n",
    "    c for c in df_all.columns if c not in [\"_user_id\", \"_item_id\", RATING_COL]\n",
    "]\n",
    "\n",
    "# Reorders the columns to ensure that _user_id, _item_id, and Book-Rating are the first three columns\n",
    "df_all = (\n",
    "    df_all.select([\"_user_id\", \"_item_id\", RATING_COL] + df_all_columns)\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "display(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9912f-1f7c-4df3-97da-188db84dd551",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Let's count and print the total number of distinct users, the total number of distinct books, and the total number of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aecab5-c642-4cc4-9c97-d126485c6c04",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total Users: {df_users.select('_user_id').distinct().count()}\")\n",
    "print(f\"Total Items: {df_items.select('_item_id').distinct().count()}\")\n",
    "print(f\"Total User-Item Interactions: {df_all.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed7d2b-957e-46ed-b9da-07816573d80c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e408ef3-fe7f-45df-98a0-7f648c1c2f4d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Let's compute the most popular books and then display the top 10. Note that the `<topn>` popular items can be used for **\"Popular\"** or **\"Most purchased\"** recommendation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d503e-b7b9-41c0-98f8-72627f102a4a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Compute top popular products\n",
    "df_top_items = (\n",
    "    df_all.groupby([\"_item_id\"])\n",
    "    .count()\n",
    "    .join(df_items, \"_item_id\", \"inner\")\n",
    "    .sort([\"count\"], ascending=[0])\n",
    ")\n",
    "\n",
    "# Find top <topn> popular items\n",
    "topn = 10\n",
    "pd_top_items = df_top_items.limit(topn).toPandas()\n",
    "pd_top_items.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7b7f5-2a26-424a-8c72-94b6ccefc41e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot top <topn> items\n",
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "sns.barplot(y=ITEM_INFO_COL, x=\"count\", data=pd_top_items)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.xlabel(\"Number of Ratings for the Item\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c82a22-f57b-4b98-8c64-1fd5e1ff1261",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "This concludes the brief analysis of the three datasets where you added unique IDs to the user and item datasets and plotted the top items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587d7e8-b907-4e98-b52f-15e94a7788d1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Data Preparation for Machine Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178ac19-a670-4ec6-9825-9421c6f9d62f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Prepare training and testing datasets\n",
    "\n",
    "Prior to training, you need to perform some additional data preparation steps for the ALS recommender. First cast the rating column into the correct type and then sample the training data with user ratings. Once done with data preparation, split the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38a006-521e-46cd-ac4b-ea4cf7a6fe69",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if IS_SAMPLE:\n",
    "    # Must sort by '_user_id' before performing limit to ensure ALS work normally\n",
    "    # Note that if train and test datasets have no common _user_id, ALS will fail\n",
    "    df_all = df_all.sort(\"_user_id\").limit(SAMPLE_ROWS)\n",
    "\n",
    "# Cast column into the correct type\n",
    "df_all = df_all.withColumn(RATING_COL, F.col(RATING_COL).cast(\"float\"))\n",
    "\n",
    "# Using a fraction between 0 to 1 returns the approximate size of the dataset, i.e., 0.8 means 80% of the dataset\n",
    "# Rating = 0 means the user didn't rate the item, so it can't be used for training\n",
    "# We use the 80% if the dataset with rating > 0 as the training dataset\n",
    "fractions_train = {0: 0}\n",
    "fractions_test = {0: 0}\n",
    "for i in ratings:\n",
    "    if i == 0:\n",
    "        continue\n",
    "    fractions_train[i] = 0.8\n",
    "    fractions_test[i] = 1\n",
    "# training dataset\n",
    "train = df_all.sampleBy(RATING_COL, fractions=fractions_train)\n",
    "\n",
    "# Join with leftanti will select all rows from df_all with rating > 0 and not in train dataset, i.e., the remaining 20% of the dataset\n",
    "# test dataset\n",
    "test = df_all.join(train, on=\"id\", how=\"leftanti\").sampleBy(\n",
    "    RATING_COL, fractions=fractions_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c12b1-050d-4404-8643-25dbe8472cc2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Also compute the sparsity of the dataset to gain a better understanding of the data and the problem at hand. Sparsity refers to the  situation in which feedback data is sparse and insufficient to identify similarities in users' interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90f521-bffe-4820-b9ae-45c26389e9ae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Compute the sparsity of the dataset\n",
    "def get_mat_sparsity(ratings):\n",
    "    # Count the total number of ratings in the dataset - used as numerator\n",
    "    count_nonzero = ratings.select(RATING_COL).count()\n",
    "    print(f\"Number of rows: {count_nonzero}\")\n",
    "\n",
    "    # Count the total number of distinct user_id and distinct product_id - used as denominator\n",
    "    total_elements = (\n",
    "        ratings.select(\"_user_id\").distinct().count()\n",
    "        * ratings.select(\"_item_id\").distinct().count()\n",
    "    )\n",
    "\n",
    "    # Calculate the sparsity by dividing the numerator by the denominator\n",
    "    sparsity = (1.0 - (count_nonzero * 1.0) / total_elements) * 100\n",
    "    print(\"The ratings dataframe is \", \"%.4f\" % sparsity + \"% sparse.\")\n",
    "\n",
    "get_mat_sparsity(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adf414-50fe-42ad-9de1-e14a4f1ab5b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Check the id range\n",
    "# Note that ALS only supports values in the integer range\n",
    "print(f\"max user_id: {df_all.agg({'_user_id': 'max'}).collect()[0][0]}\")\n",
    "print(f\"max item_id: {df_all.agg({'_item_id': 'max'}).collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762eb40-c336-4212-8de0-e09d16b23aa4",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Develop and Train the Model\n",
    "\n",
    "With our data in place, you can now define the model. Train an Alternating Least Squares (ALS) recommender to give users personalized recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ddf1e-3e14-4fa5-8f2a-fea5417bf37a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### The Machine Learning Model\n",
    "\n",
    "With the train and test datasets defined, you can now get started on the recommendation model. Spark ML provides a convenient API for building an ALS model. However, the model default setting does not perform well at handling data sparsity and the cold start problem (the difficulty to make recommendations when the users or the items are new).\n",
    "Combine cross validation and auto hyperparameter tuning to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fce1a-0f33-4611-9e24-d318cbd3d992",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You need to import the required libraries for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ccb09-ead9-46e3-a77f-fdbf848e26ce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark required libraries\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ed994-723a-4f4f-b918-a630f7eaa58b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Specify the training parameters\n",
    "num_epochs = 1  # number of epochs, here we use 1 to reduce the training time\n",
    "rank_size_list = [64]  # the values of rank in ALS for tuning\n",
    "reg_param_list = [0.01, 0.1]  # the values of regParam in ALS for tuning\n",
    "model_tuning_method = \"TrainValidationSplit\"  # TrainValidationSplit or CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a52b2-de98-4200-aed8-dad8913b5123",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note that we set the cold start strategy to 'drop' to ensure that we don't get NaN evaluation metrics\n",
    "als = ALS(\n",
    "    maxIter=num_epochs,\n",
    "    userCol=\"_user_id\",\n",
    "    itemCol=\"_item_id\",\n",
    "    ratingCol=RATING_COL,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    implicitPrefs=False,\n",
    "    nonnegative=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7afa4-4038-4577-9852-84ba035de2fc",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Model Training and hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a59137-9f69-45b1-84e3-ce1cc2f64de5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In the following code, you construct a grid of parameters to search over the hyper-parameters as well as a regression evaluator with root mean square error as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7da60-2690-46ed-bdac-e46293c4d661",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#  Construct a grid search to select the best values for the training parameters\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(als.rank, rank_size_list)\n",
    "    .addGrid(als.regParam, reg_param_list)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"Number of models to be tested: \", len(param_grid))\n",
    "\n",
    "# Define evaluator and set the loss fucntion to root mean squared error (RMSE) \n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=RATING_COL, predictionCol=\"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873316a-af0b-4899-8719-896c87e9cf18",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Initiate different model tuning methods based on the pre-configured parameters. Further information of model tuning can be found at [spark.apache.org](https://spark.apache.org/docs/latest/ml-tuning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c1ff2-244d-488b-93fd-72ddc661d203",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Build cross validation using CrossValidator and TrainValidationSplit\n",
    "if model_tuning_method == \"CrossValidator\":\n",
    "    tuner = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5,\n",
    "        collectSubModels=True,\n",
    "    )\n",
    "elif model_tuning_method == \"TrainValidationSplit\":\n",
    "    tuner = TrainValidationSplit(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        # 80% of the training data will be used for training, 20% for validation\n",
    "        trainRatio=0.8,\n",
    "        collectSubModels=True,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model_tuning_method: {model_tuning_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19bbe76-3dbc-411a-8bd4-9450ab4f822e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Model Evaluation\n",
    "\n",
    "You now have different models to compare by evaluating them on the test data. If a model has been well trained, it should have high metrics on the datasets.\n",
    "\n",
    "If the model is overfitted, you may need to increase the size of the training data or reduce some of the redundant features. You may also need to change the model's architecture or fine-tune its hyperparameters.\n",
    "\n",
    "Note that if the R-Squared metric value is negative, it indicates that the trained model performs worse than a horizontal straight line, suggesting that the data is not explained by the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f3031-73db-4e24-875d-5bae32871819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, verbose=0):\n",
    "    \"\"\"\n",
    "    Evaluate the model by computing rmse, mae, r2 and variance over the data.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = model.transform(data).withColumn(\n",
    "        \"prediction\", F.col(\"prediction\").cast(\"double\")\n",
    "    )\n",
    "\n",
    "    if verbose > 1:\n",
    "        # Show 10 predictions\n",
    "        predictions.select(\"_user_id\", \"_item_id\", RATING_COL, \"prediction\").limit(\n",
    "            10\n",
    "        ).show()\n",
    "\n",
    "    # Initialize the regression evaluator\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=RATING_COL)\n",
    "\n",
    "    _evaluator = lambda metric: evaluator.setMetricName(metric).evaluate(predictions)\n",
    "    rmse = _evaluator(\"rmse\")\n",
    "    mae = _evaluator(\"mae\")\n",
    "    r2 = _evaluator(\"r2\")\n",
    "    var = _evaluator(\"var\")\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"RMSE score = {rmse}\")\n",
    "        print(f\"MAE score = {mae}\")\n",
    "        print(f\"R2 score = {r2}\")\n",
    "        print(f\"Explained variance = {var}\")\n",
    "\n",
    "    return predictions, (rmse, mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ed2ed-6bf2-4a92-b395-e938422b526f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Experiment Tracking with MLFlow\n",
    "\n",
    "Start the training and evaluation, then use MLFlow to track all experiments and log parameters, metrics, and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d917f-b9d2-4336-8c36-f1bfae30bade",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "with mlflow.start_run(run_name=\"als\"):\n",
    "    # Train models\n",
    "    models = tuner.fit(train)\n",
    "    best_metrics = {\"RMSE\": 10e6, \"MAE\": 10e6, \"R2\": 0, \"Explained variance\": 0}\n",
    "    best_index = 0\n",
    "    # Evaluate models\n",
    "    # Log model, metrics and parameters\n",
    "    for idx, model in enumerate(models.subModels):\n",
    "        with mlflow.start_run(nested=True, run_name=f\"als_{idx}\") as run:\n",
    "            print(\"\\nEvaluating on testing data:\")\n",
    "            print(f\"subModel No. {idx + 1}\")\n",
    "            predictions, (rmse, mae, r2, var) = evaluate(model, test, verbose=1)\n",
    "\n",
    "            signature = infer_signature(\n",
    "                train.select([\"_user_id\", \"_item_id\"]),\n",
    "                predictions.select([\"_user_id\", \"_item_id\", \"prediction\"]),\n",
    "            )\n",
    "            print(\"log model:\")\n",
    "            mlflow.spark.log_model(\n",
    "                model,\n",
    "                f\"{EXPERIMENT_NAME}-alsmodel\",\n",
    "                signature=signature,\n",
    "                registered_model_name=f\"{EXPERIMENT_NAME}-alsmodel\",\n",
    "                dfs_tmpdir=\"Files/spark\",\n",
    "            )\n",
    "            print(\"log metrics:\")\n",
    "            current_metric = {\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"R2\": r2,\n",
    "                \"Explained variance\": var,\n",
    "            }\n",
    "            mlflow.log_metrics(current_metric)\n",
    "            if rmse < best_metrics[\"RMSE\"]:\n",
    "                best_metrics = current_metric\n",
    "                best_index = idx\n",
    "\n",
    "            print(\"log parameters:\")\n",
    "            mlflow.log_params(\n",
    "                {\n",
    "                    \"subModel_idx\": idx,\n",
    "                    \"num_epochs\": num_epochs,\n",
    "                    \"rank_size_list\": rank_size_list,\n",
    "                    \"reg_param_list\": reg_param_list,\n",
    "                    \"model_tuning_method\": model_tuning_method,\n",
    "                    \"DATA_FOLDER\": DATA_FOLDER,\n",
    "                }\n",
    "            )\n",
    "    # Log best model and related metrics and parameters to the parent run\n",
    "    mlflow.spark.log_model(\n",
    "        models.subModels[best_index],\n",
    "        f\"{EXPERIMENT_NAME}-alsmodel\",\n",
    "        signature=signature,\n",
    "        registered_model_name=f\"{EXPERIMENT_NAME}-alsmodel\",\n",
    "        dfs_tmpdir=\"Files/spark\",\n",
    "    )\n",
    "    mlflow.log_metrics(best_metrics)\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"subModel_idx\": idx,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"rank_size_list\": rank_size_list,\n",
    "            \"reg_param_list\": reg_param_list,\n",
    "            \"model_tuning_method\": model_tuning_method,\n",
    "            \"DATA_FOLDER\": DATA_FOLDER,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34b00f-2f22-4aaf-aaec-1397c1e529b1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "To view the logged information for the training run, select the experiment named `aisample-recommendation` from your workspace. If you changed the experiment name, select the experiment with the name you specified. The logged information appears similar to the following images. Any experiment with its respective name is logged and you can track its parameters and performance metrics: \n",
    "\n",
    "<img style=\"float: left;\" src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Book-Recommendation-Dataset/book-mlflow.png\"  width=\"45%\" height=\"10%\">\n",
    "<img style=\"float: left;\" src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Book-Recommendation-Dataset/book-logging.png\"  width=\"45%\" height=\"10%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674d0ce-4bac-4a83-825b-a0e892e3608f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Load the final model for scoring and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad60fc-7445-4253-bb45-898dc4f12e79",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Offline Recommendation\n",
    "Once done with the training and having selected the best model, load the final model for scoring and generate predictions to recommend the top 10 books for each user. Also present the recommendations in an interpretable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e29d5-bfd2-4a25-a76e-7e033cafb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "# Note that mlflow uses the PipelineModel to wrap the original model, thus we extract the original ALSModel from the stages\n",
    "model_uri = f\"models:/{EXPERIMENT_NAME}-alsmodel/1\"\n",
    "loaded_model = mlflow.spark.load_model(model_uri, dfs_tmpdir=\"Files/spark\").stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b4290-5c72-492e-9c0b-4dc488bc068a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Generate top 10 book recommendations for each user\n",
    "userRecs = loaded_model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e75b41-0db0-4682-873d-f48138b74a5b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Represent the recommendations in an interpretable format\n",
    "userRecs = (\n",
    "    userRecs.withColumn(\"rec_exp\", F.explode(\"recommendations\"))\n",
    "    .select(\"_user_id\", F.col(\"rec_exp._item_id\"), F.col(\"rec_exp.rating\"))\n",
    "    .join(df_items.select([\"_item_id\", \"Book-Title\"]), on=\"_item_id\")\n",
    ")\n",
    "userRecs.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae886a11-4390-4cd9-8559-18296cc22829",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You have received the top 10 book recommendations for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8dfafe-c21b-4277-bed4-cee5e827b006",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Save the predictions to the Lakehouse\n",
    "\n",
    "Finally, write the recommendation generated by the best ALS model back to the Lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c64a3-8ea2-4ffe-873b-386b63f1050c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Code to save the userRecs into lakehouse\n",
    "userRecs.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    f\"{DATA_FOLDER}/predictions/userRecs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36469bab-7db2-4b84-bf35-d320ba48ae7b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Full run cost {int(time.time() - ts)} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
